{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "借助 Intel(R) Extension for Scikit-learn，您可以加速您的 Scikit-learn 应用程序，并且仍然完全符合所有 Scikit-Learn API 和算法。这是一款免费软件 AI 加速器，可为各种应用带来超过10-100 倍的加速。而且您甚至不需要更改现有代码！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn-intelex\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM ,Conv1D,Dropout,Bidirectional,Multiply\n",
    "from keras.models import Model\n",
    "# from attention_utils import get_activations\n",
    "import  pandas as pd\n",
    "import  numpy as np\n",
    "\n",
    "\n",
    "import keras.backend as K \n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder,MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
    "    # Documentation is available online on Github at the address below.\n",
    "    # From: https://github.com/philipperemy/keras-visualize-activations\n",
    "    print('----- activations -----')\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "        if print_shape_only:\n",
    "            print(layer_activations.shape)\n",
    "        else:\n",
    "            print('shape为',layer_activations.shape)\n",
    "            print(layer_activations)\n",
    "    return activations\n",
    "\n",
    "\n",
    "def get_data(n, input_dim, attention_column=1):\n",
    "    \"\"\"\n",
    "    Data generation. x is purely random except that it's first value equals the target y.\n",
    "    In practice, the network should learn that the target = x[attention_column].\n",
    "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
    "    :param n: the number of samples to retrieve.\n",
    "    :param input_dim: the number of dimensions of each element in the series.\n",
    "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
    "    :return: x: model inputs, y: model targets\n",
    "    \"\"\"\n",
    "    x = np.random.standard_normal(size=(n, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column] = y[:, 0]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_data_recurrent(n, time_steps, input_dim, attention_column=10):\n",
    "    \"\"\"\n",
    "    Data generation. x is purely random except that it's first value equals the target y.\n",
    "    In practice, the network    should learn that the target = x[attention_column].\n",
    "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
    "    :param n: the number of samples to retrieve.\n",
    "    :param time_steps: the number of time steps of your series.\n",
    "    :param input_dim: the number of dimensions of each element in the series.\n",
    "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
    "    :return: x: model inputs, y: model targets\n",
    "    \"\"\"\n",
    "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column, :] = np.tile(y[:], (1, input_dim))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_data_recurrent2(n, time_steps, input_dim, attention_dim=5):\n",
    "    \"\"\"\n",
    "    Suppose input_dim = 10  time_steps = 6\n",
    "    formed one  x 6 x 10 The data of each step 6 dimension is the same as y\n",
    "    \"\"\"\n",
    "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:,:,attention_dim] =  np.tile(y[:], (1, time_steps))\n",
    "\n",
    "\n",
    "    return x,y\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = inputs\n",
    "    #a = Permute((2, 1))(inputs)\n",
    "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(input_dim, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((1, 2), name='attention_vec')(a)\n",
    "\n",
    "    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "# Another way of writing the attention mechanism is suitable for the use of the above error source:https://blog.csdn.net/uhauha2929/article/details/80733255\n",
    "def attention_3d_block2(inputs, single_attention_vector=False):\n",
    "    # If the upper layer is LSTM, you need return_sequences=True\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    time_steps = K.int_shape(inputs)[1]\n",
    "    input_dim = K.int_shape(inputs)[2]\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    if single_attention_vector:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    # Multiplied by the attention weight, but there is no summation, it seems to have little effect\n",
    "    # If you classify tasks, you can do Flatten expansion\n",
    "    # element-wise\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(dataset:pd.DataFrame, look_back:int):\n",
    "    '''\n",
    "    Processing the data\n",
    "    '''\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset.iloc[i:(i+look_back),1:]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset.iloc[i + look_back,:])\n",
    "    TrainX = np.array(dataX)\n",
    "    Train_Y = np.array(dataY)\n",
    "\n",
    "    return TrainX, Train_Y\n",
    "\n",
    "\n",
    "\n",
    "def attention_model():\n",
    "    # inputs = Input(((data.shape[1],1)))\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIMS))\n",
    "    x = Conv1D(filters = 64,kernel_size =32,dilation_rate=2,padding=\"same\", activation=\"relu\")(inputs)\n",
    "    # x = Conv1D(filters = 64, kernel_size = 1, activation = 'relu')(inputs)  #, padding = 'same'\n",
    "    x = Dropout(drop)(x)\n",
    "\n",
    "    #lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
    "    #For GPU you can use CuDNNLSTM\n",
    "    lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True,name='bilstm'))(x)\n",
    "    lstm_out = Dropout(drop)(lstm_out)\n",
    "\n",
    "    #attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(lstm_out)\n",
    "\n",
    "    #output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    output = Dense(1, activation='linear')(attention_mul)\n",
    "    model = Model(inputs=[inputs], outputs=output)\n",
    "    return model\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n",
    "# keras.layers.Activation('linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dfstate = pd.read_csv(\"./data/states.txt\")\n",
    "dfstate[\"start\"] = dfstate.apply(\n",
    "    lambda row: row[\"date\"] + \" \" + row[\"start_time\"], axis=1\n",
    ")\n",
    "dfstate[\"end\"] = dfstate.apply(lambda row: row[\"date\"] + \" \" + row[\"end_time\"], axis=1)\n",
    "dfstate[\"label\"] = dfstate.pop(\"state\").str.strip()\n",
    "# 转为时间类型\n",
    "dfstate[[\"start\", \"end\"]] = dfstate[[\"start\", \"end\"]].apply(pd.to_datetime)\n",
    "# dfstate[\"statev\"] = dfstate.apply(label_to_number, axis=1)\n",
    "# dfstate['label'] = dfstate.apply(number_to_label,axis=1)\n",
    "dfstate.to_csv(\"./data/states.csv\")\n",
    "print(dfstate.head(), dfstate.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义读取，处理数据函数\n",
    "def fix_data(inputDF: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 创建一个新的空列用于存储结果\n",
    "\n",
    "    inputDF.insert(0, \"label\", \"-\")\n",
    "\n",
    "    # 对df1中的每一行遍历，并查找df2中符合条件的记录\n",
    "    for index, row in inputDF.iterrows():\n",
    "        condition = (dfstate[\"start\"] <= row[\"时间\"]) & (row[\"时间\"] <= dfstate[\"end\"])\n",
    "        match = dfstate[condition]\n",
    "\n",
    "        if not match.empty:\n",
    "            # 如果找到了匹配项，则将df2的'state'赋值给df1的新列\n",
    "            inputDF.at[index, \"label\"] = match[\"label\"].values[0]\n",
    "        else:\n",
    "            # 如果没有找到匹配项，则保持原样（这里已经初始化为-）\n",
    "            pass\n",
    "\n",
    "    # inputDF.insert(1,'state',0)\n",
    "    # inputDF[\"state\"] = inputDF[\"label\"].astype(\"category\").cat.codes\n",
    "    return inputDF\n",
    "\n",
    "\n",
    "def read_data(file_path: str) -> pd.DataFrame:\n",
    "    return fix_data(\n",
    "        pd.read_excel(file_path, engine=\"openpyxl\", parse_dates=[\"时间\", \"轨迹时间\"])\n",
    "    )\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "df17 = read_data(\"./data/data_17.xlsx\")\n",
    "df21 = read_data(\"./data/data_21.xlsx\")\n",
    "df22 = read_data(\"./data/data_22.xlsx\")\n",
    "df23 = read_data(\"./data/data_23.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.concat([df17, df21], axis=0, ignore_index=True).drop([ \"轨迹时间\"], axis = 1)\n",
    "number_types = [np.number]\n",
    "label_types = [\"object\"]\n",
    "time_types = [\"datetime64[ns]\"]\n",
    "numeric_cols = data.select_dtypes(include=number_types).columns.tolist()\n",
    "label_cols = data.select_dtypes(include=label_types).columns.tolist()\n",
    "time_cols = data.select_dtypes(include=time_types).columns.tolist()\n",
    "print(numeric_cols,label_cols,time_cols)\n",
    "print(data.columns)\n",
    "print(data.shape)\n",
    "\n",
    "df22= df22.drop([ \"轨迹时间\"], axis = 1)\n",
    "df23= df23.drop([ \"轨迹时间\"], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data.plot(legend=True, subplots=True, figsize=(30,30),  xlabel=\"steps\")\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(left=0, right=19000)\n",
    "\n",
    "# 或者直接指定字体文件路径\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 系统自带的黑体\n",
    "plt.rcParams['font.serif'] = ['SimSun']  # 系统自带的宋体\n",
    "plt.rcParams['font.family'] = ['SimSun','SimHei','Source Han Sans CN'] \n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改各自层数！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def normalize(inpudf: pd.DataFrame,numeric_cols:list,label_cols:list,time_cols:list) -> Tuple[pd.DataFrame , StandardScaler,LabelEncoder]:\n",
    "\n",
    "    # 使用 StandardScaler 或者 MinMaxScaler 进行标准化处理。\n",
    "    # 创建标准化器对象\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(inpudf[numeric_cols])\n",
    "\n",
    "    # （label）标准化\n",
    "    le = LabelEncoder()\n",
    "    #[\"装\", \"卸\",'-']\n",
    "    le.fit(inpudf[label_cols])\n",
    "\n",
    "    # newdata=np.array(inpudf)\n",
    "    # 数值型数据标准化： 使用StandardScaler或者MinMaxScaler进行标准化处理。\n",
    "    for col in numeric_cols:\n",
    "        inpudf[col] = le.fit_transform(inpudf[col])\n",
    "\n",
    "    # 类别数据（label）标准化： 类别数据通常不做标准化，而是进行编码，比如One-Hot编码或者Label Encoding。\n",
    "    for col in label_cols:\n",
    "        inpudf[col] = le.fit_transform(inpudf[col])\n",
    "    # 时间数据标准化： 时间数据通常不进行标准化，而是转换成可以比较的形式，如Unix时间戳或时间段差。\n",
    "    # 将所有时间列转换为Unix时间戳（单位为秒）\n",
    "    for col in time_cols:\n",
    "        inpudf[col] = (\n",
    "            inpudf[col].astype(np.int64) / 1e9\n",
    "        )  # 默认pandas的datetime64[ns]转换为Unix时间戳（秒）\n",
    "\n",
    "    return inpudf,scaler,le\n",
    "\n",
    "\n",
    "# 反标准化\n",
    "\n",
    "\n",
    "def denormalize(inpudf: pd.DataFrame,scaler:StandardScaler,le:LabelEncoder) -> pd.DataFrame:\n",
    "    \"\"\"对df中的数值列进行反标准化处理\"\"\"\n",
    "\n",
    "    # 数值型数据标准化-\n",
    "    inpudf[numeric_cols] = scaler.inverse_transform(inpudf[numeric_cols])\n",
    "\n",
    "    # 类别数据（label）标准化-\n",
    "    for col in label_cols:\n",
    "        inpudf[col] = le.inverse_transform(inpudf[col])\n",
    "\n",
    "    # 时间数据标准化-\n",
    "    for col in time_cols:\n",
    "        inpudf[col] = pd.to_datetime(\n",
    "            inpudf[col] * 1e9\n",
    "        )  # 默认pandas的datetime64[ns]转换为Unix时间戳（秒）\n",
    "\n",
    "    return inpudf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMS = len(numeric_cols)+ len(time_cols)\n",
    "TIME_STEPS = 50\n",
    "lstm_units = 64\n",
    "drop = 0\n",
    "numeric_cols = [ '原始重量', '稳定重量', '轴重', 'su_ad', \n",
    "                'wpu_x', 'wpu_y', 'wpu_z',\n",
    "                'su_x', 'su_y', 'su_z', \n",
    "                '原始重量_diff', '稳定重量_diff', '轴重_diff',\n",
    "                'su_ad_diff', 'wpu_x_diff', 'wpu_y_diff', 'wpu_z_diff',\n",
    "                'su_x_diff','su_y_diff', 'su_z_diff', \n",
    "                '速度', '估计重量']\n",
    "\n",
    "# ['原始重量', '稳定重量', '轴重', 'su_ad', \n",
    "#                 'wpu_x', 'wpu_y', 'wpu_z', \n",
    "#                 'su_x', 'su_y', 'su_z', \n",
    "#                 '原始重量_diff', '稳定重量_diff', '轴重_diff', \n",
    "#                 'su_ad_diff', 'wpu_x_diff', 'wpu_y_diff', 'wpu_z_diff',\n",
    "#                 'su_x_diff', 'su_y_diff', 'su_z_diff', \n",
    "#                 '速度', '估计重量', '原始重量加速度', '稳定重量加速度', '原始重量加速度_二阶', '稳定重量加速度_二阶'] \n",
    "label_cols = ['label']\n",
    "time_cols = ['时间']\n",
    "#Normalized\n",
    "data,scaler,le = normalize(data,numeric_cols,label_cols,time_cols)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras \n",
    "X = data[numeric_cols]\n",
    "Y = data['label']\n",
    "# display(X)\n",
    "# print(Y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.asarray(X), np.asarray(Y), test_size=0.33, shuffle= True)\n",
    "\n",
    "# The known number of output classes.\n",
    "num_classes = 3\n",
    "colums_num = 22 # len(numeric_cols)\n",
    "print(len(numeric_cols))\n",
    "# Input image dimensions\n",
    "input_shape = (colums_num,1)\n",
    "\n",
    "# Convert class vectors to binary class matrices. This uses 1 hot encoding.\n",
    "y_train_binary = keras.utils.to_categorical(y_train,num_classes)\n",
    "y_test_binary = keras.utils.to_categorical(y_test,num_classes)\n",
    " \n",
    "x_train = x_train.reshape(len(x_train), colums_num,1)\n",
    "x_test = x_test.reshape(len(x_test), colums_num,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, (3), input_shape=(TIME_STEPS, INPUT_DIMS), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.Adadelta(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "# model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = colums_num\n",
    "epochs = 100\n",
    "history = model.fit(x_train, y_train_binary,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(history.history)\n",
    "plt.plot( history.history['loss'])\n",
    "plt.plot( history.history['val_loss'])\n",
    "plt.plot( history.history['val_accuracy'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'epoch','accuracy'], loc='upper right')\n",
    "plt.show()\n",
    "plt.plot( history.history['accuracy'])\n",
    "plt.plot( history.history['val_accuracy'])\n",
    "plt.title('model train vs validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'val_accuracy'], loc='upper left')\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df22.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df22,scaler22,le22 = normalize(df22,numeric_cols,label_cols,time_cols)\n",
    "df23,scaler23,le23 = normalize(df23,numeric_cols,label_cols,time_cols)\n",
    "\n",
    "x_df22 = df22[numeric_cols]\n",
    "y_df22 = df22['label']\n",
    "y_df22 = keras.utils.to_categorical(np.asfarray(y_df22),num_classes) \n",
    "x_df22=np.asfarray(x_df22)\n",
    "x_df22 = x_df22.reshape(len(x_df22), colums_num,1)\n",
    "results22 = model.predict(x_df22)\n",
    "display(results22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_acc = plt.figure(figsize=(9, 9))\n",
    "plt.plot(results22[0:27000])\n",
    "plt.plot(y_df22[0:27000])\n",
    "plt.title('real vs pred')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('epoch')\n",
    "plt.xlim((19000, 23000))\n",
    "plt.legend(['pred', 'real'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('以下是CNN_biLSTM_Attention误差')\n",
    "print('R^2决定系数：',r2_score(test_Y[000:14000],results[00:14000]))\n",
    "print('RMSE为：',np.sqrt(mean_squared_error(test_Y[2500:18000],results[2500:18000])))\n",
    "print('MAPE为：',(abs(results[9000:15000] -test_Y[9000:15000])/ test_Y[9000:15000]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new=[]\n",
    "predict_new=[] \n",
    "\n",
    "for k in range(len(results)):\n",
    "    if test_Y[k]!=0:\n",
    "        test_new.append(test_Y[k])\n",
    "        predict_new.append(results[k])\n",
    "\n",
    "def MAPE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.fabs((y_true - y_pred) / y_true))\n",
    "mape = format(MAPE(test_new, predict_new), '.4f') \n",
    "mape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
