{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ebbd1c72261"
      },
      "source": [
        "借助 Intel(R) Extension for Scikit-learn，您可以加速您的 Scikit-learn 应用程序，并且仍然完全符合所有 Scikit-Learn API 和算法。这是一款免费软件 AI 加速器，可为各种应用带来超过10-100 倍的加速。而且您甚至不需要更改现有代码！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee2f9bed4b6c"
      },
      "outputs": [],
      "source": [
        "\n",
        "   ## !python -m pip install pandas matplotlib scikit-learn-intelex scikit-learn openpyxl tensorboard seaborn ipykernel ipywidgets tensorflow mypy keras\n",
        " #tensorflow[and-cuda]\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb5fa4ce7567"
      },
      "outputs": [],
      "source": [
        "# 本导入顺序可以看到类型\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "# import tensorflow_datasets as tfds\n",
        "import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearnex import patch_sklearn\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import preprocessing as p\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# from tensorflow import keras\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import backend as K\n",
        "\n",
        "# K =keras.backend\n",
        "\n",
        "patch_sklearn()\n",
        "# from attention_utils import get_activations\n",
        "\n",
        "#更好地兼容 Python 3 的行为和特性，使得代码可以在 Python 2 和 Python 3 下运行得更加一致\n",
        "# from __future__ import absolute_import,division,print_function,unicode_literals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4644b11cec9d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "print(' ')\n",
        "print(f'{datetime.datetime.now()} tensorflow版本:', tf.__version__)\n",
        "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "print(' ')\n",
        "print('gpus： ')\n",
        "print(gpus)\n",
        "print(' ')\n",
        "print('gpus')\n",
        "# 查看系统中可见的GPU设备\n",
        "print(\"Available GPU devices:\", tf.config.list_physical_devices(\"GPU\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb9b4891a771"
      },
      "outputs": [],
      "source": [
        "refeshdata = False\n",
        "\n",
        "categories = [\"-\", \"装\", \"卸\"]\n",
        "\n",
        "\n",
        "def convert_to_int(data):\n",
        "    return categories.index(data)\n",
        "\n",
        "\n",
        "# 确保'label'列是一个分类类型（如果还不是的话）\n",
        "# if not isinstance(df_selected[\"label\"].dtype, pd.CategoricalDtype):\n",
        "#     df_selected[\"label\"] = df_selected[\"label\"].astype(\"category\")\n",
        "\n",
        "if refeshdata or 'dfstate' not in locals() or (hasattr(locals()['dfstate'], 'empty') and locals()['dfstate'].empty):\n",
        "    dfstate = pd.read_csv(\"./data/states.txt\")\n",
        "    dfstate[\"start\"] = dfstate.apply(lambda row: row[\"date\"] + \" \" + row[\"start_time\"], axis=1)\n",
        "    dfstate[\"end\"] = dfstate.apply(lambda row: row[\"date\"] + \" \" + row[\"end_time\"], axis=1)\n",
        "    dfstate[\"label\"] = dfstate.pop(\"state\").str.strip().apply(convert_to_int)\n",
        "\n",
        "    # 转为时间类型\n",
        "    dfstate[[\"start\", \"end\"]] = dfstate[[\"start\", \"end\"]].apply(pd.to_datetime)\n",
        "    # dfstate[\"statev\"] = dfstate.apply(label_to_number, axis=1)\n",
        "    # dfstate['label'] = dfstate.apply(number_to_label,axis=1)\n",
        "    dfstate.to_csv(\"./data/states.csv\")\n",
        "    # print(dfstate.head(), dfstate.shape)\n",
        "\n",
        "dfstate.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7703ad44ca0a"
      },
      "outputs": [],
      "source": [
        "dfstate.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57924df583c1"
      },
      "outputs": [],
      "source": [
        "# 定义读取数据函数\n",
        "def fix_data(inputDF: pd.DataFrame) -> pd.DataFrame:\n",
        "    # 创建一个新的空列用于存储结果\n",
        "    inputDF.insert(0, \"before\", False)\n",
        "    inputDF.insert(0, \"after\", False)\n",
        "    inputDF.insert(0, \"label\", 0)\n",
        "    # 对df1中的每一行遍历，并查找df2中符合条件的记录\n",
        "    for index, row in inputDF.iterrows():\n",
        "\n",
        "        condition = (dfstate[\"start\"] <= row[\"时间\"]) & (row[\"时间\"] <= dfstate[\"end\"])\n",
        "        match = dfstate[condition]\n",
        "        if not match.empty:\n",
        "            # 如果找到了匹配项，则将df2的'state'赋值给df1的新列\n",
        "            inputDF.at[index, \"label\"] = match[\"label\"].values[0]\n",
        "        else:\n",
        "            # 如果没有找到匹配项，则保持原样（这里已经初始化为-）\n",
        "            pass\n",
        "\n",
        "    # inputDF.insert(1,'state',0)\n",
        "    # inputDF[\"state\"] = inputDF[\"label\"].astype(\"category\").cat.codes\n",
        "    return inputDF\n",
        "\n",
        "\n",
        "def read_data(file_path: str) -> pd.DataFrame:\n",
        "    return fix_data(pd.read_excel(file_path, engine=\"openpyxl\", parse_dates=[\"时间\", \"轨迹时间\"]))\n",
        "\n",
        "\n",
        "# 读取数据\n",
        "if (refeshdata or \"odf17\" not in locals() or (hasattr(locals()[\"odf17\"], \"empty\") and locals()[\"odf17\"].empty)):\n",
        "    # 如果df未定义或为空DataFrame\n",
        "    odf17 = read_data(\"./data/data_17.xlsx\")\n",
        "    odf21 = read_data(\"./data/data_21.xlsx\")\n",
        "    odf22 = read_data(\"./data/data_22.xlsx\")\n",
        "    odf23 = read_data(\"./data/data_23.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "output_folder=\"./model_output\"\n",
        "\n",
        "#用来保存模型以及我们需要的所有东西\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "save_format=\"hdf5\" #或saved_model\n",
        "if save_format==\"hdf5\":\n",
        "    save_path_models=os.path.join(output_folder,\"hdf5_models\")\n",
        "    if not os.path.exists(save_path_models):\n",
        "        os.makedirs(save_path_models)\n",
        "    model_save_path=os.path.join(save_path_models,\"ckpt_epoch{epoch:02d}_val_acc{val_accuracy:.2f}.hdf5\")\n",
        "    \n",
        "elif save_format==\"saved_model\":\n",
        "    save_path_models=os.path.join(output_folder,\"saved_models\")\n",
        "    if not os.path.exists(save_path_models):\n",
        "        os.makedirs(save_path_models)\n",
        "    model_save_path=os.path.join(save_path_models,\"ckpt_epoch{epoch:02d}_val_acc{val_accuracy:.2f}.ckpt\")\n",
        "#用来保存日志\n",
        "log_dir= os.path.join(output_folder,'logs_{}'.format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2de8d92ca35"
      },
      "source": [
        "组合数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d4da807abc1",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "numeric_col_names = ['原始重量', '稳定重量', '轴重', 'su_ad', 'wpu_x', 'wpu_y', 'wpu_z', 'su_x', 'su_y', 'su_z']\n",
        "# delete some\n",
        "#  \"轨迹时间\",'原始重量_diff', '稳定重量_diff', '轴重_diff','su_ad_diff', 'wpu_x_diff', 'wpu_y_diff', 'wpu_z_diff','估计重量',    'su_x_diff','su_y_diff', 'su_z_diff',  ,\n",
        "# ['原始重量', '稳定重量', '轴重', 'su_ad',\n",
        "#                 'wpu_x', 'wpu_y', 'wpu_z',\n",
        "#                 'su_x', 'su_y', 'su_z',\n",
        "#                 '原始重量_diff', '稳定重量_diff', '轴重_diff',\n",
        "#                 'su_ad_diff', 'wpu_x_diff', 'wpu_y_diff', 'wpu_z_diff',\n",
        "#                 'su_x_diff', 'su_y_diff', 'su_z_diff',\n",
        "#                 '速度', '估计重量', '原始重量加速度', '稳定重量加速度', '原始重量加速度_二阶', '稳定重量加速度_二阶']\n",
        "label_col_names = [\"label\"]\n",
        "time_col_names = [\"时间\"]\n",
        "feature_col_names = numeric_col_names  # 暂时去掉 时间\n",
        "feature_col_num = len(feature_col_names)\n",
        "\n",
        "class_num = len(categories)\n",
        "# print(class_num)\n",
        "time_steps = 3\n",
        "# batch_size = 20\n",
        "batch_size = 5\n",
        "epochs = 500\n",
        "lr_decay_epochs=1\n",
        "\n",
        "all_col_names = numeric_col_names + label_col_names + time_col_names\n",
        "df17, df21, df22, df23 = odf17[all_col_names], odf21[all_col_names], odf22[all_col_names], odf23[all_col_names]\n",
        "_origindata = pd.concat([df17, df21, df22], axis=0, ignore_index=True)\n",
        "\n",
        "print(_origindata.columns)\n",
        "\n",
        "# _origindata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bfd36101289"
      },
      "source": [
        "筛选数据，"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af1ac0413f37"
      },
      "outputs": [],
      "source": [
        "def expandRows(idx_list_list, windowsize=20) -> np.ndarray:\n",
        "    my_array = np.arange(-windowsize, windowsize + 1)\n",
        "    new_array = idx_list_list.copy()\n",
        "    for item in my_array:\n",
        "        new_array += [element + item for element in indices_list]\n",
        "    ret = np.sort(list(dict.fromkeys(new_array)))\n",
        "\n",
        "    return ret\n",
        "\n",
        "\n",
        "indices_list = _origindata.index[_origindata['label'] > 0].tolist(\n",
        ")  # + _origindata.index[_origindata['label'] == '卸'].tolist()\n",
        "indices_list = expandRows(indices_list)\n",
        "\n",
        "# 或者使用.iloc基于位置索引（如果是整数索引）\n",
        "# df_selected = _origindata.iloc[indices_list]\n",
        "\n",
        "df_selected =_origindata\n",
        "\n",
        "df_selected.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7842983d4a4f"
      },
      "outputs": [],
      "source": [
        "# df_selected[feature_col_names + label_col_names].plot(legend=True, subplots=True, figsize=(30, 30), xlabel=\"steps\")\n",
        "# ax = plt.gca()\n",
        "# # ax.set_xlim(left=0, right=7600)\n",
        "# # 或者直接指定字体文件路径\n",
        "# # plt.rcParams['font.sans-serif'] = ['DejaVu Sans']  # 系统自带的黑体\n",
        "# # plt.rcParams['font.serif'] = ['DejaVu Sans']  # 系统自带的宋体\n",
        "# # plt.rcParams['font.family'] = ['DejaVu Sans']\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ad7bf83f17b"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51a1b3bf8319"
      },
      "outputs": [],
      "source": [
        "# 归一化函数\n",
        "def normalize(inpudf: pd.DataFrame, numeric_col_names: list[str]) -> np.ndarray:\n",
        "    # 使用 StandardScaler 或者 MinMaxScaler 进行标准化处理。\n",
        "    # 创建标准化器对象\n",
        "    scaler = p.MinMaxScaler()\n",
        "    return scaler.fit_transform(inpudf)\n",
        "    # for col in numeric_col_names :\n",
        "    #     result[col]=scaler.fit_transform(np.asarray(inpudf[col] ))\n",
        "\n",
        "\n",
        "# def normalizeY(y: pd.DataFrame):\n",
        "#     \"\"\"对df中的数值列进行反标准化处理\"\"\"\n",
        "#     # 定义一个字典，将标签映射到整数值\n",
        "#     custom_label_dict = {    '-': 0, '卸': 1, '装': 2, }\n",
        "#     le = OneHotEncoder(categories='auto', sparse=False)\n",
        "#     print('mapping ---->',custom_label_dict)\n",
        "#     # 现在我们可以将ydata转换为数值\n",
        "#     ydata_numeric = y.apply(lambda x: custom_label_dict[x['label']],axis=1)\n",
        "#     return ydata_numeric\n",
        "def normalizeY(y: pd.DataFrame):\n",
        "    \"\"\"对df中的数值列进行反标准化处理\"\"\"\n",
        "    # 定义一个字典，将标签映射到整数值\n",
        "    ydata = np.asarray(y[label_col_names])\n",
        "    le = p.OneHotEncoder()\n",
        "    le.fit(ydata)\n",
        "\n",
        "    return le.transform(ydata), le\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4140f6f52fdc"
      },
      "outputs": [],
      "source": [
        "# class weights\n",
        "\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "\n",
        "def generate_class_weights(class_series:np.ndarray, multi_class=True, one_hot_encoded=False):\n",
        "  \"\"\"\n",
        "  Method to generate class weights given a set of multi-class or multi-label labels, both one-hot-encoded or not.\n",
        "  Some examples of different formats of class_series and their outputs are:\n",
        "    - generate_class_weights(['mango', 'lemon', 'banana', 'mango'], multi_class=True, one_hot_encoded=False)\n",
        "    {'banana': 1.3333333333333333, 'lemon': 1.3333333333333333, 'mango': 0.6666666666666666}\n",
        "    - generate_class_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], multi_class=True, one_hot_encoded=True)\n",
        "    {0: 0.6666666666666666, 1: 1.3333333333333333, 2: 1.3333333333333333}\n",
        "    - generate_class_weights([['mango', 'lemon'], ['mango'], ['lemon', 'banana'], ['lemon']], multi_class=False, one_hot_encoded=False)\n",
        "    {'banana': 1.3333333333333333, 'lemon': 0.4444444444444444, 'mango': 0.6666666666666666}\n",
        "    - generate_class_weights([[0, 1, 1], [0, 0, 1], [1, 1, 0], [0, 1, 0]], multi_class=False, one_hot_encoded=True)\n",
        "    {0: 1.3333333333333333, 1: 0.4444444444444444, 2: 0.6666666666666666}\n",
        "  The output is a dictionary in the format { class_label: class_weight }. In case the input is one hot encoded, the class_label would be index\n",
        "  of appareance of the label when the dataset was processed. \n",
        "  In multi_class this is np.unique(class_series) and in multi-label np.unique(np.concatenate(class_series)).\n",
        "  Author: Angel Igareta (angel@igareta.com)\n",
        "  \"\"\"\n",
        "  if multi_class:\n",
        "    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n",
        "    if one_hot_encoded:\n",
        "      class_series = np.argmax(class_series, axis=1)\n",
        "  \n",
        "    # Compute class weights with sklearn method\n",
        "    class_labels = np.unique(class_series)\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n",
        "    return dict(zip(class_labels, class_weights))\n",
        "  else:\n",
        "    # It is neccessary that the multi-label values are one-hot encoded\n",
        "    mlb = None\n",
        "    if not one_hot_encoded:\n",
        "      mlb = MultiLabelBinarizer()\n",
        "      class_series = mlb.fit_transform(class_series)\n",
        "\n",
        "    n_samples = len(class_series)\n",
        "    n_classes = len(class_series[0])\n",
        "\n",
        "    # Count each class frequency\n",
        "    # class_count = [0] * n_classes\n",
        "    # for classes in class_series:\n",
        "    #     for index in range(n_classes):\n",
        "    #         if classes[index] != 0:\n",
        "    #             class_count[index] += 1\n",
        "    \n",
        "    class_count = np.array(class_series).sum(axis=0)\n",
        "    \n",
        "    # Compute class weights using balanced method\n",
        "    class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\n",
        "    class_labels = range(len(class_weights)) if mlb is None else mlb.classes_\n",
        "    return dict(zip(class_labels, class_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 准备数据\n",
        "def windowed_dataset(dataset: tf.data.Dataset, window_size=5, shift=1, stride=1):\n",
        "    windows = dataset.window(window_size, shift=shift, stride=stride, drop_remainder=True)\n",
        "\n",
        "    def sub_to_batch(t1, t2=None):\n",
        "        t1_batches = t1.batch(window_size, drop_remainder=True)\n",
        "\n",
        "        if t2 is not None:\n",
        "            t2_batches = t2.batch(window_size, drop_remainder=True)\n",
        "            return tf.data.Dataset.zip(t1_batches, t2_batches)\n",
        "        else:\n",
        "            return t1_batches\n",
        "\n",
        "    windows = windows.flat_map(sub_to_batch)\n",
        "    return windows\n",
        "\n",
        "\n",
        "def one_hot_encoding(label):\n",
        "    return tf.squeeze(tf.one_hot(label, depth=class_num))\n",
        "\n",
        "\n",
        "y = tf.data.Dataset.from_tensor_slices(df_selected[label_col_names]).map(one_hot_encoding)\n",
        "\n",
        "test_weight = generate_class_weights(list(y), multi_class=True, one_hot_encoded=True)\n",
        "\n",
        "\n",
        "# 定义用于归一化/标准化的函数\n",
        "def normalize_feature1(value):\n",
        "    mean = tf.reduce_mean(value)\n",
        "    std = tf.math.reduce_std(value)\n",
        "    return (value - mean) / std\n",
        "\n",
        "\n",
        "def normalize_feature2(value):\n",
        "    min_val = tf.reduce_min(value)\n",
        "    max_val = tf.reduce_max(value)\n",
        "    return (value - min_val) / (max_val - min_val)\n",
        "\n",
        "\n",
        "# 使用map函数对不同特征列进行相应的归一化\n",
        "# normalized_dataset = dataset.map(\n",
        "#     lambda feats: {\n",
        "#         'feature1': normalize_feature1(feats['feature1']),\n",
        "#         'feature2': normalize_feature2(feats['feature2']),\n",
        "#     })\n",
        "# for col in feature_col_names:\n",
        "#     df_selected[col] =scaler.fit_transform(df_selected[col])\n",
        "\n",
        "scaler = p.StandardScaler()\n",
        "normalized = scaler.fit_transform(df_selected[feature_col_names])\n",
        "\n",
        "x = tf.data.Dataset.from_tensor_slices(normalized)\n",
        "featureTarget = windowed_dataset(x, time_steps)\n",
        "\n",
        "target = tf.data.Dataset.zip(featureTarget, y)\n",
        "\n",
        "total_examples = len(x)  # 获取数据集总样本数\n",
        "train_ratio = 0.6  # 训练集占比\n",
        "num_train_examples = int(total_examples * train_ratio)\n",
        "num_val_examples = total_examples - num_train_examples\n",
        "train_dataset = target.take(num_train_examples)\n",
        "val_dataset = target.skip(num_train_examples).take(num_val_examples)\n",
        "\n",
        "trainWindow = train_dataset.batch(batch_size=batch_size)\n",
        "valWindow = val_dataset.batch(batch_size=batch_size)\n",
        "\n",
        "print(test_weight)\n",
        "for x, y in trainWindow.take(1):\n",
        "    print('x=======================')\n",
        "    print(x.numpy())\n",
        "    print('y---------------------')\n",
        "    print(y.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1382e9d4c3f"
      },
      "source": [
        "# Embedding的一些关键点\n",
        "\n",
        "Embedding是一种在深度学习中常用的技术，它将输入数据（如单词、图像、句子等）映射到低维度的向量空间中。这种映射使得原始数据可以被模型更好地处理和理解。在自然语言处理（NLP）中，嵌入通常用于将单词或字符转换为固定大小的向量，这些向量可以捕捉词汇的语义和上下文信息\n",
        "\n",
        "1.  **维度 reduction**: 嵌入将高维数据（如词汇表中的单词）映射到低维空间，使得模型可以更容易地处理。\n",
        "    \n",
        "2.  **固定大小**: 嵌入向量通常是固定大小的，这使得它们可以被矩阵操作，便于模型处理。\n",
        "    \n",
        "3.  **非线性**: 嵌入通常是通过对输入数据进行非线性变换来生成的，这有助于模型捕捉复杂的关系。\n",
        "    \n",
        "4.  **预训练和微调**: 在NLP中，单词嵌入通常预先训练然后在特定任务上微调。预训练可以捕获通用语义信息，而微调可以捕捉任务特定的信息。\n",
        "    \n",
        "5.  **神经网络**: 嵌入经常与神经网络一起使用，特别是卷积神经网络（CNN）和循环神经网络（RNN），以处理复杂的输入数据。\n",
        "    \n",
        "6.  **应用**: 除了NLP，嵌入技术还广泛应用于计算机视觉（如图像嵌入）、音频处理和其他领域。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89a8e11726cb",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "## 模型创建\n",
        "from os import name\n",
        "\n",
        "SINGLE_ATTENTION_VECTOR = False\n",
        "\n",
        "\n",
        "def attention_3d_block(inputs):\n",
        "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
        "    input_dim = int(inputs.shape[2])\n",
        "    a = inputs\n",
        "    # a = Permute((2, 1))(inputs)\n",
        "    # a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
        "    a = layers.Dense(input_dim, activation='softmax')(a)\n",
        "    if SINGLE_ATTENTION_VECTOR:\n",
        "        a = layers.Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
        "        a = layers.RepeatVector(input_dim)(a)\n",
        "    a_probs = layers.Permute((1, 2), name='attention_vec')(a)  # 维数转置\n",
        "\n",
        "    output_attention_mul = layers.concatenate([inputs, a_probs], axis=-1)  # 把两个矩阵拼接\n",
        "    return output_attention_mul\n",
        "\n",
        "\n",
        "regularizer = keras.regularizers.l2(0.01)\n",
        "\n",
        "\n",
        "def attention_model(time_steps: int, input_dims: int, lstm_units: int, output_dim: int) -> models.Model:\n",
        "    inputs = layers.Input(shape=(time_steps, input_dims), name='input')\n",
        "    x = layers.Dense(units=1024, name='dense1', activation='relu', kernel_initializer='HeUniform')(inputs)\n",
        "    x = layers.Conv1D(filters=512, kernel_size=1, activation='relu', kernel_initializer='HeUniform')(inputs)  # , padding = 'same'\n",
        "    x = layers.MaxPool1D(pool_size=2)(x)\n",
        "    # x = layers.Dropout(0.3)(x)\n",
        "    for i in range(10):\n",
        "        x = layers.Dense(units=1024, activation='relu', kernel_regularizer=regularizer, kernel_initializer='HeUniform', name='Dense_a_' + str(i + 2))(x)\n",
        "    # lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
        "    # 对于GPU可以使用CuDNNLSTM\n",
        "    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True, name='lstm1'))(x)\n",
        "    x = layers.Dropout(0.3, name='dorpot1')(x)\n",
        "    x = attention_3d_block(x)\n",
        "    # x = layers.MultiHeadAttention(num_heads=3,\n",
        "    #                               key_dim=64,\n",
        "    #                               value_dim=64,\n",
        "    #                               name='attention')(x)\n",
        "    for i in range(2):\n",
        "        x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True, name='lstm' + str(i + 2)))(x)\n",
        "        x = layers.Dropout(0.3, name='dorpot' + str(i + 2))(x)\n",
        "    for i in range(10):\n",
        "        x = layers.Dense(units=1024, activation='relu', kernel_regularizer=regularizer, kernel_initializer='HeUniform', name='Dense_b_' + str(i + 2))(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    # attention_mul = layers.Embedding(input_dims, output_dim)(attention_mul)\n",
        "    output = layers.Dense(output_dim, activation='softmax', name='output_')(x)\n",
        "    model = keras.Model(inputs=[inputs], outputs=output)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "376f0adbc276"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 编译模型 Old\n",
        "# drop = 0.2\n",
        "\n",
        "# # 调整学习率\n",
        "# optimizer = keras.optimizers.Adam(learning_rate=0.2)  # 学习率设置为0.001\n",
        "\n",
        "# # class_num=1\n",
        "# lstm_units = 64\n",
        "\n",
        "# model = attention_model(time_steps, feature_col_num, lstm_units, class_num)\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer=optimizer,\n",
        "#     loss=keras.losses.CategoricalCrossentropy(),\n",
        "#     metrics=['accuracy'],\n",
        "#     #   steps_per_execution=5\n",
        "# )\n",
        "\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f2793cf74af",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "# 编译模型\n",
        "# class_num=1\n",
        "lstm_units = 64\n",
        "model = attention_model(time_steps, feature_col_num, lstm_units, class_num)\n",
        "\n",
        "#学习率变化设置，使用指数衰减\n",
        "train_steps_per_epoch = int(total_examples // batch_size)\n",
        "initial_learning_rate = 0.01\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
        "#                                                              decay_steps=1*train_steps_per_epoch,\n",
        "#                                                             decay_rate=0.96,\n",
        "#                                                             staircase=True)#initial_learning_rate*0.96**(step/decay_steps)\n",
        "#优化算法\n",
        "optimizer = keras.optimizers.SGD(learning_rate=initial_learning_rate, momentum=0.95)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "#损失函数\n",
        "loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "#评价指标\n",
        "# metrics=[keras.metrics.SparseCategoricalAccuracy(),loss]\n",
        "metrics = ['accuracy']  #, 'categorical_crossentropy'\n",
        "\n",
        "# 标签平滑损失函数\n",
        "loss = keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
        "#上式第二个参数会返回交叉熵的结果，用loss减去该值就会得到正则化的值（于model.losses和相等），这两种定义方式都可以，下边的会显示名称短一些\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#模型保存格式默认是saved_model,可以自己定义更改原有类来保存hdf5\n",
        "ckpt = keras.callbacks.ModelCheckpoint(model_save_path.replace('.hdf5', '.keras'), monitor='val_accuracy', verbose=1, save_best_only=False, save_weights_only=False, save_freq='epoch', mode='auto')\n",
        "#当模型训练不符合我们要求时停止训练，连续5个epoch验证集精度没有提高0.001%停\n",
        "earlystop = keras.callbacks.EarlyStopping(\n",
        "    monitor='accuracy',  # 监控的量，val_loss, val_acc, loss, acc\n",
        "    min_delta=0,  # 监控量的变化量，当大于该值时，认为模型在性能上没有提升\n",
        "    patience=10,  # 当patience个epoch内，监控量没有提升时，停止训练\n",
        "    verbose=1,  # 0,1,2,3,4,5,6,7,8,9,10,11,12,1\n",
        "    mode='accuracy',  # auto, min, max    monitor='val_accuracy',\n",
        "    baseline=None,  # 基准线，当monitor达到baseline时，patience计数器清零\n",
        "    restore_best_weights=True,  # 是否恢复训练时验证集上表现最好的权重\n",
        ")\n",
        "#3、自定义学习率按需衰减，并把整个学习率变化过程保存\n",
        "class LearningRateExponentialDecay:\n",
        "\n",
        "    def __init__(self, initial_learning_rate, decay_epochs, decay_rate):\n",
        "        self.initial_learning_rate = initial_learning_rate\n",
        "        self.decay_epochs = decay_epochs\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "    def __call__(self, epoch):\n",
        "        dtype = type(self.initial_learning_rate)\n",
        "        decay_epochs = np.array(self.decay_epochs).astype(dtype)\n",
        "        decay_rate = np.array(self.decay_rate).astype(dtype)\n",
        "        epoch = np.array(epoch).astype(dtype)\n",
        "        p = epoch / decay_epochs\n",
        "        lr = self.initial_learning_rate * np.power(decay_rate, p)\n",
        "        return lr\n",
        "\n",
        "lr_schedule = LearningRateExponentialDecay(initial_learning_rate, lr_decay_epochs, 0.96)\n",
        "lr = keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1)\n",
        "#使用tensorboard\n",
        "#定义当loss出现nan或inf时停止训练的callback\n",
        "terminate = keras.callbacks.TerminateOnNaN()\n",
        "\n",
        "#模型损失长时间不除时大程度降低学习率\n",
        "# 这个策略通常不于学习率衰减schedule同时使用，或者使用时要合理\n",
        "#降低学习率（要比学习率自动周期变化有更大变化和更长时间监控）\n",
        "# 模型损失长时间不除时大程度降低学习率\n",
        "# 这个策略通常不于学习率衰减schedule同时使用，或者使用时要合理\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                              factor=0.1,\n",
        "                                              patience=3,\n",
        "                                              verbose=1,\n",
        "                                              min_delta=0.0001,\n",
        "                                              min_lr=0)\n",
        "\n",
        "#保存训练过程中大数标量指标，与tensorboard同一个文件\n",
        "csv_logger = keras.callbacks.CSVLogger(os.path.join(log_dir, 'logs.log'), separator=',')\n",
        "\n",
        "#还要加入tensorboard的使用,这种方法记录的内容有限\n",
        "#各个参数的作用请参看文档，需要正确使用\n",
        "tensorboard = keras.callbacks.TensorBoard(\n",
        "    log_dir=log_dir,\n",
        "    histogram_freq=1,  #对参数和激活做直方图，一定要有测试集\n",
        "    write_graph=True,  #模型结构图\n",
        "    write_images=True,  #把模型参数做为图片形式存到\n",
        "    update_freq='epoch',  #epoch,batch,整数，太频的话会减慢速度\n",
        "    profile_batch=2,  #记录模型性能\n",
        "    embeddings_freq=1,\n",
        "    embeddings_metadata=None  #这个还不太清楚如何使用\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "callbacks = [ckpt, earlystop, lr, tensorboard, terminate, reduce_lr, csv_logger]\n",
        "\n",
        "history = model.fit(\n",
        "    trainWindow,\n",
        "    epochs=epochs,\n",
        "    verbose='auto',\n",
        "    # batch_size=batch_size,\n",
        "    validation_data=valWindow,\n",
        "    # validation_split=0.2,\n",
        "    # shuffle=False,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=test_weight,\n",
        "    # workers=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e735d350cea5"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir \"./logs\"\n",
        "# cmd 当前环境，当前目录 运行   tensorboard --logdir=logs --host=127.0.0.1\n",
        "# 网页中可以查看模型训练过程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff7fe6ec6f7f"
      },
      "outputs": [],
      "source": [
        "H = history\n",
        "\n",
        "\n",
        "#画学习率变化曲线并保存到log中\n",
        "def plot(lrs, title=\"Learning Rate Schedule\"):\n",
        "    #计算学习率随epoch的变化值\n",
        "    epochs = np.arange(len(lrs))\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, lrs)\n",
        "    plt.xticks(epochs)\n",
        "    plt.scatter(epochs, lrs)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Learning Rate\")\n",
        "\n",
        "\n",
        "plot(H.history['lr'])\n",
        "plt.savefig(os.path.join(log_dir, 'learning_rate.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存模型结构及配置参数\n",
        "model_json = model.to_json()\n",
        "with open(os.path.join(log_dir,'model_json.json'),'w') as json_file:\n",
        "    json_file.write(model_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#对模型在测试集上进行评估\n",
        "metrics = model.evaluate(valWindow, verbose=1)\n",
        "print(\"val_loss:\", metrics[0], \"val_accuracy:\", metrics[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions=model.predict(valWindow,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "332c1750c3e9"
      },
      "outputs": [],
      "source": [
        "# # data.plot(legend=True, subplots=True, figsize=(30,30),  xlabel=\"steps\")\n",
        "# # ax = plt.gca()\n",
        "# # ax.set_xlim(left=0, right=19000)\n",
        "\n",
        "# # # 或者直接指定字体文件路径\n",
        "# # plt.rcParams['font.sans-serif'] = ['SimHei']  # 系统自带的黑体\n",
        "# # plt.rcParams['font.serif'] = ['SimSun']  # 系统自带的宋体\n",
        "# # plt.rcParams['font.family'] = ['SimSun','SimHei','Source Han Sans CN']\n",
        "\n",
        "# # plt.show()\n",
        "# times = df22[time_col_names].index.to_numpy()\n",
        "# display(times)\n",
        "# plt.plot(times, y, label=\"Actual\", marker=\"o\")\n",
        "# plt.plot(times, predictions, label=\"Predicted\", linestyle=\"--\", marker=\"x\")\n",
        "\n",
        "# ax = plt.gca()\n",
        "# ax.set_xlim(left=12000, right=18000)\n",
        "# # 添加标题和标签\n",
        "# plt.title(\"Actual vs. Predicted Values\")\n",
        "# plt.xlabel(\"Sample Index\")\n",
        "# plt.ylabel(\"Value\")\n",
        "\n",
        "# # 添加图例\n",
        "# plt.legend()\n",
        "\n",
        "# # 显示图形\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3777035567f2"
      },
      "outputs": [],
      "source": [
        "# display(history.history)\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"model train vs validation loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"loss\", \"epoch\", \"accuracy\"], loc=\"upper right\")\n",
        "plt.show()\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"model train vs validation accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"accuracy\", \"val_accuracy\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9ca4cfdba43"
      },
      "outputs": [],
      "source": [
        "display(df22.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4072f6e1f1a6"
      },
      "outputs": [],
      "source": [
        "# data22, scaler22 = normalize(df22, numeric_col_names)\n",
        "# X22 = data22[feature_col_names]\n",
        "# y_test22 =normalizeY( df22[label_col_names])\n",
        "# x_train22  = np.asarray(X22) .reshape(-1, feature_col_num)\n",
        "# predY22 = model.predict(x_train22, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# data23, scaler23 = normalize(df23, numeric_col_names)\n",
        "# X23 = data23[feature_col_names]\n",
        "# y_test23 =normalizeY( df23[label_col_names])\n",
        "# x_train23  = np.asarray(X23) .reshape(-1, feature_col_num)\n",
        "# predY23 = model.predict(x_train23, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# display(predY22,predY23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e692ce96c13"
      },
      "outputs": [],
      "source": [
        "# fig_acc = plt.figure(figsize=(9, 9))\n",
        "# plt.plot(predY23[0:27000])\n",
        "# plt.plot(y_test23[0:27000])\n",
        "# plt.title(\"real vs pred\")\n",
        "# plt.ylabel(\"value\")\n",
        "# plt.xlabel(\"epoch\")\n",
        "# plt.xlim((19000, 23000))\n",
        "# plt.legend([\"pred\", \"real\"], loc=\"upper left\")\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "C_L_A_train_test.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
