{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee2f9bed4b6c"
      },
      "outputs": [],
      "source": [
        "#借助 Intel(R) Extension for Scikit-learn，您可以加速您的 Scikit-learn 应用程序，并且仍然完全符合所有 Scikit-Learn API 和算法。这是一款免费软件 AI 加速器，可为各种应用带来超过10-100 倍的加速。而且您甚至不需要更改现有代码！\n",
        "# !python -m pip install pandas matplotlib scikit-learn-intelex scikit-learn openpyxl tensorboard seaborn ipykernel ipywidgets keras plotly plotly_express\n",
        "# !python -m pip install  tensorflow==2.15.*\n",
        "# !python -m pip install  tensorflow[and-cuda]==2.15.*\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb5fa4ce7567"
      },
      "outputs": [],
      "source": [
        "# 本导入顺序可以看到类型\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "# import tensorflow_datasets as tfds\n",
        "import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearnex import patch_sklearn\n",
        "from sklearn import preprocessing as skl\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# from tensorflow import keras\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "\n",
        "\n",
        "patch_sklearn()\n",
        "# from attention_utils import get_activations\n",
        "\n",
        "#更好地兼容 Python 3 的行为和特性，使得代码可以在 Python 2 和 Python 3 下运行得更加一致\n",
        "# from __future__ import absolute_import,division,print_function,unicode_literals\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import ticker as mt\n",
        "# 或者直接指定字体文件路径\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans Mono']  # 系统自带的黑体\n",
        "plt.rcParams['font.serif'] = ['SimHei']  # 系统自带的宋体\n",
        "plt.rcParams['font.family'] = [\n",
        "    'SimHei',\n",
        "    'DejaVu Sans Mono',  # 显示负号的字体\n",
        "    # 'Liberation Mono',\n",
        "    'Consolas',\n",
        "    'Courier New',\n",
        "    'monospace',\n",
        "    'sans-serif',\n",
        "    'serif',\n",
        "    'sans',\n",
        "    'serif',\n",
        "    'sans-serif',\n",
        "    'sans',\n",
        "    'serif',\n",
        "    'sans-serif',\n",
        "    'sans',\n",
        "    'serif',\n",
        "    'sans-serif',\n",
        "    'sans',\n",
        "    'serif',\n",
        "    'sans-serif',\n",
        "    'sans',\n",
        "    'serif',\n",
        "    'sans-serif',\n",
        "    'sans',\n",
        "    'serif',\n",
        "]\n",
        "plt.rcParams['axes.unicode_minus'] = False  # 设置matplotlib显示正常的负号而非减号样式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4644b11cec9d"
      },
      "outputs": [],
      "source": [
        "print(' ')\n",
        "print(f'{datetime.datetime.now()} tensorflow版本:', tf.__version__)\n",
        "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "print(' ')\n",
        "print('gpus： ')\n",
        "print(gpus)\n",
        "print(' ')\n",
        "print('gpus')\n",
        "# 查看系统中可见的GPU设备\n",
        "print(\"Available GPU devices:\", tf.config.list_physical_devices(\"GPU\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 读取数据\n",
        "\n",
        "from load_data import load_fixed_data, categories\n",
        "\n",
        "R_CREATE = False\n",
        "R_CREATE = True\n",
        "\n",
        "\n",
        "def create_dynamic_globals(dyn_name, value, global_vars, read_cache: bool = True):\n",
        "    \"\"\"\n",
        "    :param dyn_name: 动态变量名\n",
        "    :param value: 动态变量值\n",
        "    :param global_vars: 全局变量字典\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # print(dyn_name, global_vars[dyn_name])\n",
        "    if read_cache and (R_CREATE or dyn_name not in global_vars or global_vars[dyn_name] == None):\n",
        "        # 如果确实需要在全局作用域创建真正的全局变量，可以使用exec，但这不是推荐做法\n",
        "        if callable(value):\n",
        "\n",
        "            v = value(dyn_name)\n",
        "            s = f'global {dyn_name}; {dyn_name} = v'\n",
        "            # print(s)\n",
        "            exec(s)\n",
        "        else:\n",
        "            exec(f'global {dyn_name}; {dyn_name} = {value}')\n",
        "\n",
        "    return globals()[dyn_name]\n",
        "\n",
        "\n",
        "def create_global_vars(global_vars, varprefix: str = 'data_', read_cache: bool = True):\n",
        "\n",
        "    for gvar in global_vars:\n",
        "        #\n",
        "        if gvar.startswith('_'):\n",
        "            continue\n",
        "        if gvar.startswith(varprefix):\n",
        "            print(gvar, )\n",
        "            create_dynamic_globals(gvar, lambda f: load_fixed_data(f'./data/{f}.xlsx'), global_vars)\n",
        "\n",
        "\n",
        "data_17 = pd.DataFrame()\n",
        "data_21 = data_17\n",
        "data_22 = data_17\n",
        "data_23 = data_17\n",
        "data_24 = data_17\n",
        "\n",
        "create_global_vars(globals(), varprefix='data_')\n",
        "\n",
        "# data_17.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from libs.config import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d4da807abc1",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "#组合数据集\n",
        "from load_data import categories\n",
        "\n",
        "numeric_col_names = [\n",
        "    '原始重量', '稳定重量', '轴重', 'su_ad', 'wpu_x', 'wpu_y', 'wpu_z', 'su_x', 'su_y', 'su_z', '原始重量_diff', '稳定重量_diff', '轴重_diff', 'su_ad_diff', 'wpu_x_diff', 'wpu_y_diff', 'wpu_z_diff', 'su_x_diff', 'su_y_diff', 'su_z_diff', '速度', '估计重量'\n",
        "]\n",
        "# delete some\n",
        "# ['Unnamed: 0' ,\n",
        "# 'label'      , '时间'         , '轨迹时间'       , '速度',, '估计重量'\n",
        "# '原始重量'       , '稳定重量'       , '轴重'         , 'su_ad',\n",
        "# '原始重量_diff'  , '稳定重量_diff'  , '轴重_diff'    , 'su_ad_diff',\n",
        "# 'wpu_x'      , 'wpu_y'      , 'wpu_z'      ,\n",
        "# 'wpu_x_diff' , 'wpu_y_diff' , 'wpu_z_diff',\n",
        "# 'su_x'       , 'su_y'       , 'su_z'       ,\n",
        "# 'su_x_diff'  , 'su_y_diff'  , 'su_z_diff'\n",
        "# ]\n",
        "label_col_names = [\"label\"]\n",
        "time_col_names = [\"时间\", \"轨迹时间\"]\n",
        "feature_col_names = numeric_col_names  # 暂时去掉 时间\n",
        "feature_col_num = len(feature_col_names)\n",
        "\n",
        "class_num = len(categories)\n",
        "\n",
        "\n",
        "all_col_names = numeric_col_names + label_col_names + time_col_names\n",
        "df17, df21, df22, df23, df24 = data_17[all_col_names], data_21[all_col_names], data_22[all_col_names], data_23[all_col_names], data_24[all_col_names]\n",
        "origindata = pd.concat([df17, df21, df22, df23, df24], axis=0, ignore_index=True)\n",
        "\n",
        "# print(origindata.columns)\n",
        "\n",
        "print(origindata.shape, origindata.columns, origindata.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check 数据集有缺失值\n",
        "shapea= origindata.shape\n",
        "df = origindata.dropna()\n",
        "shapeb =df.shape\n",
        "print(shapea, shapeb)\n",
        "if shapea != shapeb:\n",
        "    print(shapea,shapeb)\n",
        "    ValueError(\"数据集有缺失值\")\n",
        "\n",
        "# missing_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 假设df是一个时间序列DataFrame，其中索引是时间戳\n",
        "if not '时间' in df.index.names or not isinstance(df.index, pd.DatetimeIndex):\n",
        "    df.sort_values('时间', axis=0, ascending=True)\n",
        "    df.set_index('时间', inplace=True)  # 如果还没有将时间戳设为索引\n",
        "    df = df.sort_index(axis=0, ascending=True)\n",
        "# df.interpolate(method='linear', inplace=True)\n",
        "# 确保索引已经是datetime类型\n",
        "# df.index = pd.to_datetime(df.index)\n",
        "\n",
        "# 计算相邻时间点之间的差值\n",
        "time_diffs = df.index.to_series(name='timespan').diff()\n",
        "print(time_diffs)\n",
        "# 找出那些间隔超过3秒的索引\n",
        "large_gaps = time_diffs[time_diffs > pd.Timedelta(seconds=6)].index\n",
        "print(large_gaps)\n",
        "# 若要查看每段连续的间隔超过3秒的区间\n",
        "gaps_df = pd.DataFrame({'start': large_gaps[:-1], 'end': large_gaps[1:]})\n",
        "# 若只需要每个间隔的开始时间\n",
        "gap_starts = large_gaps[:-1]\n",
        "\n",
        "print(\"间隔超过3秒的开始时间点：\", gap_starts)\n",
        "time_diffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from turtle import up\n",
        "import scipy\n",
        "from scipy.stats import iqr\n",
        "\n",
        "p = 1.5\n",
        "\n",
        "\n",
        "def replace_iqr_outliers(df: pd.DataFrame, colums: list[str]):\n",
        "    q = 0.25\n",
        "    _df = df.copy()\n",
        "    for column in colums:\n",
        "        _df[column] = _df[column].astype(float)\n",
        "        q1 = _df[column].quantile(q)  #.quantile(0.25)\n",
        "        q3 = _df[column].quantile(1 - q)  #.quantile(0.75)\n",
        "        iqr_value = iqr(_df[column], rng=(q * 100, (1 - q) * 100))\n",
        "        print('------', iqr_value)\n",
        "\n",
        "        lower_bound = q1 - p * iqr_value\n",
        "        upper_bound = q3 + p * iqr_value\n",
        "        _df.loc[_df[column] < lower_bound, column] = lower_bound\n",
        "        _df.loc[_df[column] > upper_bound, column] = upper_bound\n",
        "\n",
        "    return _df\n",
        "\n",
        "\n",
        "df_iqr = replace_iqr_outliers(origindata, numeric_col_names)\n",
        "\n",
        "# df_iqr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af1ac0413f37"
      },
      "outputs": [],
      "source": [
        "from libs.expandrows import dataframe_filter\n",
        "\n",
        "df_filter_important = df_iqr  #全部数据\n",
        "\n",
        "# df_filter_important = dataframe_filter(df_iqr, 30)  # 过滤为0的数据\n",
        "\n",
        "# df_filter_important"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7842983d4a4f"
      },
      "outputs": [],
      "source": [
        "# 使用Savitzky-Golay 滤波器后得到平滑图线\n",
        "from numpy import ndarray\n",
        "from scipy import signal as sg\n",
        "\n",
        "# df_expanded = df_expanded  #.dropna()\n",
        "for col in df_filter_important[numeric_col_names].columns:\n",
        "    a = 1\n",
        "    # df_selected.insert(0,co)\n",
        "    # df_selected[col] = sg.savgol_filter(df_selected[col].astype(float), window_length=21, delta=0.5, polyorder=1, mode='constant')\n",
        "    # df_selected[col] = np.convolve(df_selected[col].astype(float), np.ones((5, )), mode='same')\n",
        "    # df_selected[col] = abs(df_selected[col].astype(float))\n",
        "    # df_selected[col] = np.convolve(df_selected[col].astype(float), np.ones((5, )), mode='same')\n",
        "\n",
        "# df_selected.sort_index(axis=1)\n",
        "\n",
        "# df_selected = df_selected.apply(lambda x: savgol_filter(x, 5, 3, mode= 'nearest'))\n",
        "#[feature_col_names + label_col_names]\n",
        "\n",
        "df_smooth = df_filter_important[feature_col_names + label_col_names + time_col_names]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python -m pip install plotly plotly_express chart-studio cufflinks pyarrow\n",
        "import plotly_express as px  # import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import chart_studio.plotly as py\n",
        "from plotly.subplots import make_subplots\n",
        "# Cufflinks wrapper on plotly\n",
        "import cufflinks as cf\n",
        "\n",
        "\n",
        "# print(df['时间'].shape, df[feature_col_names].shape)\n",
        "# fig = px.line(df, x='时间', y=feature_col_names + label_col_names,range_x=['2023-12-24 00:00:00', '2023-12-24 09:00:00'])\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "# 假设你有多个列需要比较\n",
        "columns_to_plot = feature_col_names + label_col_names\n",
        "# df_smooth.reset_index()\n",
        "df_to_plot = df_smooth[columns_to_plot]\n",
        "# dfview.reset_index(drop=True, inplace=True)\n",
        "# df_to_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51a1b3bf8319"
      },
      "outputs": [],
      "source": [
        "# 归一化函数\n",
        "def normalize(inpudf: pd.DataFrame, numeric_col_names: list[str]) -> np.ndarray:\n",
        "    # 使用 StandardScaler 或者 MinMaxScaler 进行标准化处理。\n",
        "    # 创建标准化器对象\n",
        "    scaler = skl.MinMaxScaler()\n",
        "    return scaler.fit_transform(inpudf)\n",
        "    # for col in numeric_col_names :\n",
        "    #     result[col]=scaler.fit_transform(np.asarray(inpudf[col] ))\n",
        "\n",
        "\n",
        "# def normalizeY(y: pd.DataFrame):\n",
        "#     \"\"\"对df中的数值列进行反标准化处理\"\"\"\n",
        "#     # 定义一个字典，将标签映射到整数值\n",
        "#     custom_label_dict = {    '-': 0, '卸': 1, '装': 2, }\n",
        "#     le = OneHotEncoder(categories='auto', sparse=False)\n",
        "#     print('mapping ---->',custom_label_dict)\n",
        "#     # 现在我们可以将ydata转换为数值\n",
        "#     ydata_numeric = y.apply(lambda x: custom_label_dict[x['label']],axis=1)\n",
        "#     return ydata_numeric\n",
        "def normalizeY(y: pd.DataFrame):\n",
        "    \"\"\"对df中的数值列进行反标准化处理\"\"\"\n",
        "    # 定义一个字典，将标签映射到整数值\n",
        "    ydata = np.asarray(y[label_col_names])\n",
        "    le = skl.OneHotEncoder()\n",
        "    le.fit(ydata)\n",
        "\n",
        "    return le.transform(ydata), le\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 准备数据\n",
        "\n",
        "\n",
        "def windowed_dataset(dataset: tf.data.Dataset, window_size=5, shift=1, stride=1):\n",
        "    windows = dataset.window(window_size, shift=shift, stride=stride, drop_remainder=True)\n",
        "\n",
        "    def sub_to_batch(t1, t2=None):\n",
        "        t1_batches = t1.batch(window_size, drop_remainder=True)\n",
        "\n",
        "        if t2 is not None:\n",
        "            t2_batches = t2.batch(window_size, drop_remainder=True)\n",
        "            return tf.data.Dataset.zip(t1_batches, t2_batches)\n",
        "        else:\n",
        "            return t1_batches\n",
        "\n",
        "    windows = windows.flat_map(sub_to_batch)\n",
        "    return windows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from libs.class_weight import generate_class_weights\n",
        "\n",
        "\n",
        "def one_hot_encoding(label):\n",
        "    return tf.squeeze(tf.one_hot(label, depth=class_num))\n",
        "    # print(label)\n",
        "    # return label * 100\n",
        "\n",
        "\n",
        "y = tf.data.Dataset.from_tensor_slices(df_filter_important[label_col_names].astype(int)).map(one_hot_encoding)\n",
        "\n",
        "test_weight = generate_class_weights(list(y), multi_class=True, one_hot_encoded=True)\n",
        "\n",
        "for i in y.take(3):\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "# 定义用于归一化/标准化的函数\n",
        "def normalize_feature1(value):\n",
        "    mean = tf.reduce_mean(value)\n",
        "    std = tf.math.reduce_std(value)\n",
        "    return (value - mean) / std\n",
        "\n",
        "\n",
        "def normalize_feature2(value):\n",
        "    min_val = tf.reduce_min(value)\n",
        "    max_val = tf.reduce_max(value)\n",
        "    return (value - min_val) / (max_val - min_val)\n",
        "\n",
        "\n",
        "# 使用map函数对不同特征列进行相应的归一化\n",
        "# normalized_dataset = dataset.map(\n",
        "#     lambda feats: {\n",
        "#         'feature1': normalize_feature1(feats['feature1']),\n",
        "#         'feature2': normalize_feature2(feats['feature2']),\n",
        "#     })\n",
        "# for col in feature_col_names:\n",
        "#     df_selected[col] =scaler.fit_transform(df_selected[col])\n",
        "\n",
        "scaler = skl.StandardScaler()\n",
        "normalized = scaler.fit_transform(df_filter_important[feature_col_names])\n",
        "\n",
        "x = tf.data.Dataset.from_tensor_slices(normalized)\n",
        "\n",
        "featureTarget = x\n",
        "featureTarget = windowed_dataset(x, time_steps)\n",
        "\n",
        "target = tf.data.Dataset.zip(featureTarget, y)\n",
        "\n",
        "total_examples = len(x)  # 获取数据集总样本数\n",
        "train_ratio = 0.6  # 训练集占比\n",
        "num_train_examples = int(total_examples * train_ratio)\n",
        "num_val_examples = total_examples - num_train_examples\n",
        "train_dataset = target.take(num_train_examples)\n",
        "val_dataset = target.skip(num_train_examples).take(num_val_examples)\n",
        "\n",
        "trainWindow = train_dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
        "valWindow = val_dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
        "\n",
        "print('test_weight=======================')\n",
        "print(test_weight)\n",
        "# for x, y in trainWindow.take(1):\n",
        "#     print('x=======================')\n",
        "#     print(x.numpy())\n",
        "#     print('y---------------------')\n",
        "#     print(y.numpy())\n",
        "\n",
        "# yview=np.array(list(y))\n",
        "# yview = list(y)\n",
        "\n",
        "# print(yview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensorboard --logdir=\"c:\\AppData\\code-ai\\CNN-LSTM-Attention\\model_output\\logs_20240410-175619\" --host=127.0.0.1\n"
          ]
        }
      ],
      "source": [
        "# 编译模型\n",
        "# %reload_ext autoreload\n",
        "# %autoreload 2\n",
        "from libs.callbacks import get_callbacks\n",
        "from libs.compile_model import compile_model\n",
        "\n",
        "model = compile_model(time_steps, feature_col_num, class_num, total_examples, batch_size)\n",
        "initial_learning_rate = 0.01\n",
        "callbacks = get_callbacks(initial_learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 1/5\n",
            "   6445/Unknown \u001b[1m414s\u001b[0m 63ms/step - accuracy: 0.9698 - categorical_crossentropy: 15.6215 - loss: 5.6114"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\AppData\\.devhome\\conda\\envs\\py311tf2.16\\Lib\\contextlib.py:158: UserWarning:\n",
            "\n",
            "Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: saving model to ./model_output\\hdf5_models\\ckpt_epoch01_val_acc0.97.keras\n",
            "\u001b[1m6445/6445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 69ms/step - accuracy: 0.9698 - categorical_crossentropy: 15.6215 - loss: 5.6114 - val_accuracy: 0.9667 - val_categorical_crossentropy: 15.5822 - val_loss: 15.5822 - learning_rate: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0096.\n",
            "Epoch 2/5\n",
            "\u001b[1m3132/6445\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3:03\u001b[0m 56ms/step - accuracy: 0.9632 - categorical_crossentropy: 15.5253 - loss: 5.3320"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainWindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# batch_size=batch_size,\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalWindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# validation_split=0.2,\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# shuffle=False,\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# workers=10\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\AppData\\.devhome\\conda\\envs\\py311tf2.16\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\AppData\\.devhome\\conda\\envs\\py311tf2.16\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:329\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    328\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 329\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[0;32m    331\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    332\u001b[0m     )\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
            "File \u001b[1;32mc:\\AppData\\.devhome\\conda\\envs\\py311tf2.16\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\AppData\\.devhome\\conda\\envs\\py311tf2.16\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\AppData\\.devhome\\conda\\envs\\py311tf2.16\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\AppData\\.devhome\\conda\\envs\\py311tf2.16\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\AppData\\.devhome\\conda\\envs\\py311tf2.16\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\AppData\\.devhome\\conda\\envs\\py311tf2.16\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32mc:\\AppData\\.devhome\\conda\\envs\\py311tf2.16\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
            "File \u001b[1;32mc:\\AppData\\.devhome\\conda\\envs\\py311tf2.16\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
            "File \u001b[1;32mc:\\AppData\\.devhome\\conda\\envs\\py311tf2.16\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "history = model.fit(\n",
        "    trainWindow,\n",
        "    epochs=epochs,\n",
        "    verbose='auto',\n",
        "    # batch_size=batch_size,\n",
        "    validation_data=valWindow,\n",
        "    # validation_split=0.2,\n",
        "    # shuffle=False,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=test_weight,\n",
        "    # workers=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e735d350cea5"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir \"./logs\"\n",
        "# cmd 当前环境，当前目录 运行   tensorboard --logdir=logs --host=127.0.0.1\n",
        "# 网页中可以查看模型训练过程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff7fe6ec6f7f"
      },
      "outputs": [],
      "source": [
        "H = history\n",
        "\n",
        "\n",
        "#画学习率变化曲线并保存到log中\n",
        "def plot(lrs, title=\"Learning Rate Schedule\"):\n",
        "    #计算学习率随epoch的变化值\n",
        "    epochs = np.arange(len(lrs))\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, lrs)\n",
        "    plt.xticks(epochs)\n",
        "    plt.scatter(epochs, lrs)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Learning Rate\")\n",
        "\n",
        "\n",
        "plot(H.history['lr'])\n",
        "plt.savefig(os.path.join(log_dir, 'learning_rate.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存模型结构及配置参数\n",
        "model_json = model.to_json()\n",
        "with open(os.path.join(log_dir,'model_json.json'),'w') as json_file:\n",
        "    json_file.write(model_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#对模型在测试集上进行评估\n",
        "metrics = model.evaluate(valWindow, verbose=1)\n",
        "print(\"val_loss:\", metrics[0], \"val_accuracy:\", metrics[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions=model.predict(valWindow,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 如果是多分类问题并且想比较类别概率分布\n",
        "# 假设y_true和y_pred都是one-hot编码形式\n",
        "# 可以选择几个样本画柱状图对比\n",
        "from math import e\n",
        "\n",
        "\n",
        "y_true = list(valWindow.map(lambda x, y: tf.argmax(y, axis=1)))\n",
        "y_true = tf.reshape(y_true, (-1, 1))\n",
        "# print(predictions)\n",
        "y_pred = tf.argmax(predictions, axis=1)\n",
        "for i in predictions:\n",
        "    # print(tf.argmax(i, axis=1))\n",
        "    # print(i)\n",
        "    e=1\n",
        "num_samples_to_plot = len(y_pred)\n",
        "nrows = 2\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.subplot(nrows, 1, 1)\n",
        "plt.plot(y_true, label='Actual values', linewidth=2.0)\n",
        "plt.subplot(nrows, 1, 2)\n",
        "plt.plot(y_pred, label='Predicted values', linewidth=2.0)\n",
        "plt.legend(fontsize=14)\n",
        "plt.xlabel('Sample Index')\n",
        "ax = plt.gca()\n",
        "ax.set_xlim(left=0, right=num_samples_to_plot)\n",
        "plt.ylabel('Value')\n",
        "plt.title('Actual vs Predicted Values Comparison')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "332c1750c3e9"
      },
      "outputs": [],
      "source": [
        "# # data.plot(legend=True, subplots=True, figsize=(30,30),  xlabel=\"steps\")\n",
        "# # ax = plt.gca()\n",
        "# # ax.set_xlim(left=0, right=19000)\n",
        "\n",
        "\n",
        "# # plt.show()\n",
        "# times = df22[time_col_names].index.to_numpy()\n",
        "# display(times)\n",
        "# plt.plot(times, y, label=\"Actual\", marker=\"o\")\n",
        "# plt.plot(times, predictions, label=\"Predicted\", linestyle=\"--\", marker=\"x\")\n",
        "\n",
        "# ax = plt.gca()\n",
        "# ax.set_xlim(left=12000, right=18000)\n",
        "# # 添加标题和标签\n",
        "# plt.title(\"Actual vs. Predicted Values\")\n",
        "# plt.xlabel(\"Sample Index\")\n",
        "# plt.ylabel(\"Value\")\n",
        "\n",
        "# # 添加图例\n",
        "# plt.legend()\n",
        "\n",
        "# # 显示图形\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3777035567f2"
      },
      "outputs": [],
      "source": [
        "# display(history.history)\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"model train vs validation loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"loss\", \"epoch\", \"accuracy\"], loc=\"upper right\")\n",
        "plt.show()\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"model train vs validation accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"accuracy\", \"val_accuracy\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9ca4cfdba43"
      },
      "outputs": [],
      "source": [
        "display(df22.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4072f6e1f1a6"
      },
      "outputs": [],
      "source": [
        "# data22, scaler22 = normalize(df22, numeric_col_names)\n",
        "# X22 = data22[feature_col_names]\n",
        "# y_test22 =normalizeY( df22[label_col_names])\n",
        "# x_train22  = np.asarray(X22) .reshape(-1, feature_col_num)\n",
        "# predY22 = model.predict(x_train22, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# data23, scaler23 = normalize(df23, numeric_col_names)\n",
        "# X23 = data23[feature_col_names]\n",
        "# y_test23 =normalizeY( df23[label_col_names])\n",
        "# x_train23  = np.asarray(X23) .reshape(-1, feature_col_num)\n",
        "# predY23 = model.predict(x_train23, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# display(predY22,predY23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e692ce96c13"
      },
      "outputs": [],
      "source": [
        "# fig_acc = plt.figure(figsize=(9, 9))\n",
        "# plt.plot(predY23[0:27000])\n",
        "# plt.plot(y_test23[0:27000])\n",
        "# plt.title(\"real vs pred\")\n",
        "# plt.ylabel(\"value\")\n",
        "# plt.xlabel(\"epoch\")\n",
        "# plt.xlim((19000, 23000))\n",
        "# plt.legend([\"pred\", \"real\"], loc=\"upper left\")\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "C_L_A_train_test.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
