{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee2f9bed4b6c"
      },
      "outputs": [],
      "source": [
        "#借助 Intel(R) Extension for Scikit-learn，您可以加速您的 Scikit-learn 应用程序，并且仍然完全符合所有 Scikit-Learn API 和算法。这是一款免费软件 AI 加速器，可为各种应用带来超过10-100 倍的加速。而且您甚至不需要更改现有代码！\n",
        "# !python -m pip install pandas matplotlib scikit-learn-intelex scikit-learn openpyxl tensorboard seaborn ipykernel ipywidgets keras plotly plotly_express\n",
        "# !python -m pip install  tensorflow==2.15.*\n",
        "# !python -m pip install  tensorflow[and-cuda]==2.15.*\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb5fa4ce7567"
      },
      "outputs": [],
      "source": [
        "# 本导入顺序可以看到类型\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "# import tensorflow_datasets as tfds\n",
        "import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearnex import patch_sklearn\n",
        "from sklearn import preprocessing as skl\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# from tensorflow import keras\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "\n",
        "\n",
        "patch_sklearn()\n",
        "# from attention_utils import get_activations\n",
        "\n",
        "#更好地兼容 Python 3 的行为和特性，使得代码可以在 Python 2 和 Python 3 下运行得更加一致\n",
        "# from __future__ import absolute_import,division,print_function,unicode_literals\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import ticker as mt\n",
        "# 或者直接指定字体文件路径\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans Mono']  # 系统自带的黑体\n",
        "plt.rcParams['font.serif'] = ['SimHei']  # 系统自带的宋体\n",
        "plt.rcParams['font.family'] = [\n",
        "    'SimHei',\n",
        "    'DejaVu Sans Mono',  # 显示负号的字体\n",
        "    # 'Liberation Mono',\n",
        "    'Consolas',\n",
        "    'Courier New',\n",
        "    'monospace',\n",
        "    'sans-serif',\n",
        "    'serif',\n",
        "    'sans',\n",
        "    'serif',\n",
        "    'sans-serif',\n",
        "    'sans',\n",
        "    'serif',\n",
        "    'sans-serif',\n",
        "    'sans',\n",
        "    'serif',\n",
        "    'sans-serif',\n",
        "    'sans',\n",
        "    'serif',\n",
        "    'sans-serif',\n",
        "    'sans',\n",
        "    'serif',\n",
        "    'sans-serif',\n",
        "    'sans',\n",
        "    'serif',\n",
        "]\n",
        "plt.rcParams['axes.unicode_minus'] = False  # 设置matplotlib显示正常的负号而非减号样式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4644b11cec9d"
      },
      "outputs": [],
      "source": [
        "print(' ')\n",
        "print(f'{datetime.datetime.now()} tensorflow版本:', tf.__version__)\n",
        "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "print(' ')\n",
        "print('gpus： ')\n",
        "print(gpus)\n",
        "print(' ')\n",
        "print('gpus')\n",
        "# 查看系统中可见的GPU设备\n",
        "print(\"Available GPU devices:\", tf.config.list_physical_devices(\"GPU\"))\n",
        "\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 读取数据\n",
        "\n",
        "from load_data import load_fixed_data, categories\n",
        "\n",
        "R_CREATE = False\n",
        "R_CREATE = True\n",
        "\n",
        "\n",
        "def create_dynamic_globals(dyn_name, value, global_vars, read_cache: bool = True):\n",
        "    \"\"\"\n",
        "    :param dyn_name: 动态变量名\n",
        "    :param value: 动态变量值\n",
        "    :param global_vars: 全局变量字典\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # print(dyn_name, global_vars[dyn_name])\n",
        "    if read_cache and (R_CREATE or dyn_name not in global_vars or global_vars[dyn_name] == None):\n",
        "        # 如果确实需要在全局作用域创建真正的全局变量，可以使用exec，但这不是推荐做法\n",
        "        if callable(value):\n",
        "\n",
        "            v = value(dyn_name)\n",
        "            s = f'global {dyn_name}; {dyn_name} = v'\n",
        "            # print(s)\n",
        "            exec(s)\n",
        "        else:\n",
        "            exec(f'global {dyn_name}; {dyn_name} = {value}')\n",
        "\n",
        "    return globals()[dyn_name]\n",
        "\n",
        "\n",
        "def create_global_vars(global_vars, varprefix: str = 'data_', read_cache: bool = True):\n",
        "\n",
        "    for gvar in global_vars:\n",
        "        #\n",
        "        if gvar.startswith('_'):\n",
        "            continue\n",
        "        if gvar.startswith(varprefix):\n",
        "            print(gvar, )\n",
        "            create_dynamic_globals(gvar, lambda f: load_fixed_data(f'./data/{f}.xlsx', read_from_csv=True), global_vars, read_cache)\n",
        "\n",
        "\n",
        "data_17 = pd.DataFrame()\n",
        "data_21 = data_17\n",
        "data_22 = data_17\n",
        "data_23 = data_17\n",
        "# data_24 = data_17 #无数据，不能使用\n",
        "\n",
        "create_global_vars(globals(), varprefix='data_')\n",
        "\n",
        "# data_17.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from libs.config import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d4da807abc1",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "#组合数据集\n",
        "from load_data import categories\n",
        "\n",
        "numeric_col_names = [\n",
        "    '原始重量',\n",
        "    # '稳定重量',\n",
        "    '轴重',\n",
        "    'su_ad',\n",
        "    '原始重量_diff',\n",
        "    # '稳定重量_diff',\n",
        "    '轴重_diff',\n",
        "    'su_ad_diff',\n",
        "    'wpu_x',\n",
        "    'wpu_y',\n",
        "    'wpu_z',\n",
        "    'su_x',\n",
        "    'su_y',\n",
        "    'su_z',\n",
        "    # 'su_x_diff',\n",
        "    'su_y_diff',\n",
        "    'su_z_diff',\n",
        "    'wpu_x_diff',\n",
        "    'wpu_y_diff',\n",
        "    'wpu_z_diff',\n",
        "    # '速度',\n",
        "    # '估计重量',\n",
        "]\n",
        "# numeric_col_names = [\n",
        "#     'wpu_x_diff',\n",
        "#     'wpu_y_diff',\n",
        "#     'wpu_z_diff',\n",
        "# ]\n",
        "# delete some\n",
        "# ['Unnamed: 0' ,\n",
        "# 'label'      , '时间'         , '轨迹时间'       , '速度',, '估计重量'\n",
        "# '原始重量'       , '稳定重量'       , '轴重'         , 'su_ad',\n",
        "# '原始重量_diff'  , '稳定重量_diff'  , '轴重_diff'    , 'su_ad_diff',\n",
        "# 'wpu_x'      , 'wpu_y'      , 'wpu_z'      ,\n",
        "# 'wpu_x_diff' , 'wpu_y_diff' , 'wpu_z_diff',\n",
        "# 'su_x'       , 'su_y'       , 'su_z'       ,\n",
        "# 'su_x_diff'  , 'su_y_diff'  , 'su_z_diff'\n",
        "# ]\n",
        "label_col_names = [\"label\"]\n",
        "time_col_names = [\"时间\", \"轨迹时间\"]\n",
        "\n",
        "all_col_names = numeric_col_names + label_col_names + time_col_names\n",
        "df17, df21, df22, df23 = data_17[all_col_names], data_21[all_col_names], data_22[all_col_names], data_23[all_col_names]\n",
        "\n",
        "class_num = len(categories)\n",
        "\n",
        "origindata = pd.concat([df17, df21, df22, df23], axis=0, ignore_index=True)\n",
        "\n",
        "# print(origindata.columns)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check 数据集有缺失值\n",
        "shapea= origindata.shape\n",
        "df_all_data = origindata.dropna()\n",
        "shapeb =df_all_data.shape\n",
        "print(shapea, shapeb)\n",
        "if shapea != shapeb:\n",
        "    print(shapea,shapeb)\n",
        "    ValueError(\"数据集有缺失值\")\n",
        "\n",
        "# missing_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from locale import Error\n",
        "import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from turtle import up\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from scipy.stats import iqr\n",
        "\n",
        "\n",
        "def windowed_dataset(dataset: tf.data.Dataset, window_size=5, shift=1, stride=2):\n",
        "    if window_size == None or window_size <= 0:\n",
        "        return dataset\n",
        "\n",
        "    def sub_to_batch(t1, t2=None):\n",
        "        t1_batches = t1.batch(window_size, drop_remainder=True)\n",
        "\n",
        "        if t2 is not None:\n",
        "            t2_batches = t2.batch(window_size, drop_remainder=True)\n",
        "            return tf.data.Dataset.zip((t1_batches, t2_batches))\n",
        "        else:\n",
        "            return t1_batches\n",
        "\n",
        "    def pick_batch(t1, t2: tf.Tensor | None):\n",
        "        #return (t1, t2)\n",
        "        if t2 is None:\n",
        "            return t1\n",
        "        else:\n",
        "\n",
        "            # 获取张量的维度信息\n",
        "            ndim = 0 if t2.ndim is None else t2.ndim\n",
        "\n",
        "            # 确保张量至少有二维\n",
        "            if ndim >= 2:\n",
        "                # 获取第二维的大小\n",
        "                second_dim_size = t2.shape[1] if ndim > 2 else t2.shape[0]\n",
        "                # 提取每个第二维元素的第一个数\n",
        "                # 如果张量是二维的，直接使用 tensor[:, 0]\n",
        "                # 如果张量大于二维，确保正确处理其他维度\n",
        "                if ndim > 2:\n",
        "                    first_elements = t2[0, ...]  # 使用...来保持其余维度不变\n",
        "                else:  # 对于二维张量\n",
        "                    first_elements = t2[0]\n",
        "\n",
        "                print(first_elements.shape)\n",
        "                return t1, first_elements\n",
        "            else:\n",
        "                msg = \"The tensor does not have at least two dimensions to extract elements from the second dimension.\"\n",
        "                print(msg)\n",
        "                Error(msg)\n",
        "\n",
        "    return dataset.window(window_size, shift=shift, stride=stride, drop_remainder=True).flat_map(sub_to_batch).map(pick_batch)\n",
        "    return windows  #.batch(window_size, drop_remainder=drop_remainder)\n",
        "\n",
        "\n",
        "# def windowed_batch_dataframe(df: pd.DataFrame, window_size=4, batch_size=None):\n",
        "#     df = df.copy()\n",
        "#     if batch_size is not None and batch_size > 0:\n",
        "#         dataset = windowed_dataset(tf.data.Dataset.from_tensor_slices(df.values), window_size).batch(batch_size=batch_size, drop_remainder=True)\n",
        "#     else:\n",
        "#         dataset = windowed_dataset(tf.data.Dataset.from_tensor_slices(df.values), window_size)\n",
        "#     return dataset\n",
        "\n",
        "\n",
        "def windowed_batch_dataset(ds: tf.data.Dataset, window_size=4, batch_size=None):\n",
        "\n",
        "    if batch_size is not None and batch_size > 0:\n",
        "        dataset = windowed_dataset(ds, window_size).batch(batch_size=batch_size, drop_remainder=True)\n",
        "    else:\n",
        "        dataset = windowed_dataset(ds, window_size)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def convert_to_numpy(ds: tf.data.Dataset):\n",
        "    # 将 dataset 转换为 NumPy 数组\n",
        "    numpy_array = None\n",
        "    for data in ds.as_numpy_iterator():\n",
        "        # 确保数据是 NumPy 数组\n",
        "        data_np = np.array(data)\n",
        "\n",
        "        # 如果 numpy_array 还没有初始化，初始化它\n",
        "        if numpy_array is None:\n",
        "            numpy_array = data_np\n",
        "        else:\n",
        "            # # 确保数据的形状与 numpy_array 相同\n",
        "            # if data_np.shape[1:] != numpy_array.shape[1:]:\n",
        "            #     raise ValueError(\"All data items must have the same shape except for the first dimension.\")\n",
        "\n",
        "            # 垂直堆叠数据\n",
        "            numpy_array = np.vstack((numpy_array, data_np))\n",
        "\n",
        "    return numpy_array\n",
        "\n",
        "\n",
        "def get_shape(*args):\n",
        "    shapes = []\n",
        "    for arg in args:\n",
        "        shapes.append(arg.shape if arg is not None else None)\n",
        "    return shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from turtle import up\n",
        "import scipy\n",
        "from scipy.stats import iqr\n",
        "\n",
        "p = 1.5\n",
        "\n",
        "\n",
        "def replace_iqr_outliers(df: pd.DataFrame, current_col_names: list[str], with_new_col=True):\n",
        "    q = 0.25\n",
        "    _df = df.copy()\n",
        "    numeric_iqr_col_names = [str(col + '_iqr') for col in numeric_col_names]\n",
        "    tuple_cols = zip(numeric_col_names, numeric_iqr_col_names)\n",
        "    for column, new_col in tuple_cols:\n",
        "        _df[column] = _df[column].astype(float)\n",
        "        q1 = _df[column].quantile(q)  #.quantile(0.25)\n",
        "        q3 = _df[column].quantile(1 - q)  #.quantile(0.75)\n",
        "        iqr_value = iqr(_df[column], rng=(q * 100, (1 - q) * 100))\n",
        "        # print('------', iqr_value)\n",
        "\n",
        "        lower_bound = q1 - p * iqr_value\n",
        "        upper_bound = q3 + p * iqr_value\n",
        "        if with_new_col:\n",
        "            _df[new_col] = _df[column]\n",
        "            _df.loc[_df[column] < lower_bound, new_col] = lower_bound\n",
        "            _df.loc[_df[column] > upper_bound, new_col] = upper_bound\n",
        "\n",
        "        else:\n",
        "            _df.loc[_df[column] < lower_bound, column] = lower_bound\n",
        "            _df.loc[_df[column] > upper_bound, column] = upper_bound\n",
        "    if with_new_col:\n",
        "        return _df, list(dict.fromkeys(current_col_names + numeric_iqr_col_names))\n",
        "    else:\n",
        "        return _df, current_col_names\n",
        "\n",
        "\n",
        "def abs_cols(df: pd.DataFrame, current_col_names: list[str]):\n",
        "    _df = df.copy()\n",
        "\n",
        "    numeric_abs_col_names = [str(col + '_abs') for col in numeric_col_names]\n",
        "    for numeric_col, abs_col in zip(numeric_col_names, numeric_abs_col_names):\n",
        "        df[abs_col] = abs(df[numeric_col])\n",
        "\n",
        "    return df, list(dict.fromkeys(current_col_names + numeric_abs_col_names))\n",
        "\n",
        "\n",
        "def smooth_cols(df: pd.DataFrame, current_col_names: list[str]):\n",
        "    _df = df.copy()\n",
        "\n",
        "    numeric_smooth_col_names = [str(col + '_smooth') for col in numeric_col_names]\n",
        "    for numeric_col, smooth_col in zip(numeric_col_names, numeric_smooth_col_names):\n",
        "        df[smooth_col] = abs(df[numeric_col])\n",
        "\n",
        "    return df, list(dict.fromkeys(current_col_names + numeric_smooth_col_names))\n",
        "\n",
        "\n",
        "from joblib import dump, load\n",
        "\n",
        "\n",
        "def normalize_df(alldata_df: pd.DataFrame, feature_col_names: list[str]):\n",
        "    df = alldata_df.copy()\n",
        "    # if os.path.exists('scaler.joblib'):\n",
        "    #     scaler = load('scaler.joblib')\n",
        "    #     normalized = scaler.fit_transform(df_to_plot[feature_col_names])\n",
        "    #     print('exsited scaler.joblib')\n",
        "    # else:\n",
        "    #     scaler = skl.StandardScaler()\n",
        "    #     normalized = scaler.fit_transform(df_to_plot[feature_col_names])\n",
        "    #     dump(scaler, 'scaler.joblib')\n",
        "    #     print('New scaler.joblib')\n",
        "\n",
        "    scaler = skl.StandardScaler()\n",
        "    normalized = scaler.fit_transform(df[feature_col_names])\n",
        "    # normalized = df[feature_col_names].values\n",
        "    df = pd.DataFrame(normalized, columns=feature_col_names)\n",
        "    df[label_col_names + time_col_names] = alldata_df[label_col_names + time_col_names]\n",
        "    return df, feature_col_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "# 准备数据\n",
        "# 使用Savitzky-Golay 滤波器后得到平滑图线\n",
        "from numpy import ndarray\n",
        "from scipy import signal as sg\n",
        "from libs.expandrows import dataframe_filter\n",
        "\n",
        "\n",
        "def proccess_data_df(df_inp: pd.DataFrame):\n",
        "\n",
        "    df = df_inp.copy()\n",
        "    feature_col_names = numeric_col_names\n",
        "\n",
        "    # df = dataframe_filter(df, 30)  # 过滤为0的数据\n",
        "\n",
        "    # df, feature_col_names = normalize_df(df, feature_col_names)\n",
        "\n",
        "    # print(\"间隔超过3秒的开始时间点：\", gap_starts)\n",
        "\n",
        "    df, feature_col_names = replace_iqr_outliers(df, feature_col_names, with_new_col=False)\n",
        "\n",
        "    # df, feature_col_names = smooth_cols(df, feature_col_names)\n",
        "\n",
        "    # df, feature_col_names = abs_cols(df, feature_col_names)\n",
        "\n",
        "    return df, feature_col_names, label_col_names\n",
        "\n",
        "\n",
        "def proccess_data_dataset(df_input: pd.DataFrame, epochs: int, batch_size: int):\n",
        "    import math\n",
        "    df_to_plot, feature_col_names, label_col_names = proccess_data_df(df_input)\n",
        "\n",
        "    num_examples = len(df_to_plot)\n",
        "\n",
        "    steps_per_epoch = math.floor((num_examples - time_steps + 1) / batch_size) if batch_size > 0 else 1\n",
        "\n",
        "    print('--------------------------- = ')\n",
        "    print('num_examples = ', num_examples)\n",
        "    print('batch_size = ', batch_size)\n",
        "    print('steps_per_epoch = ', steps_per_epoch)\n",
        "    print('colums = ', len(feature_col_names))\n",
        "\n",
        "    def one_hot_encoding(label):\n",
        "        return tf.squeeze(tf.one_hot(label, depth=class_num))\n",
        "\n",
        "    data = df_to_plot[feature_col_names]\n",
        "    ds_x = tf.data.Dataset.from_tensor_slices((data))\n",
        "    ds_y = tf.data.Dataset.from_tensor_slices((df_to_plot[label_col_names])).map(one_hot_encoding)\n",
        "    ds = tf.data.Dataset.zip(ds_x, ds_y)  #.repeat(epochs * steps_per_epoch)\n",
        "\n",
        "    win_ds = windowed_batch_dataset(ds, window_size=time_steps, batch_size=batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return win_ds, steps_per_epoch, feature_col_names, label_col_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python -m pip install plotly plotly_express chart-studio cufflinks pyarrow\n",
        "import plotly_express as px  # import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import chart_studio.plotly as py\n",
        "from plotly.subplots import make_subplots\n",
        "# Cufflinks wrapper on plotly\n",
        "import cufflinks as cf\n",
        "\n",
        "\n",
        "# print(df['时间'].shape, df[feature_col_names].shape)\n",
        "# fig = px.line(df, x='时间', y=feature_col_names + label_col_names,range_x=['2023-12-24 00:00:00', '2023-12-24 09:00:00'])\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from libs.class_weight import generate_class_weights\n",
        "\n",
        "\n",
        "\n",
        "# test_weight = generate_class_weights(list(y), multi_class=True, one_hot_encoded=True)\n",
        "\n",
        "test_weight = {0: 0.3434428692922046, 1: 32.374924653405664, 2: 17.415693904020753}\n",
        "\n",
        "\n",
        "# y1 = df_filter_important[label_col_names].iloc[:, 0].tolist()\n",
        "# test_weight1 = generate_class_weights(y1, multi_class=True, one_hot_encoded=False)\n",
        "# print(test_weight1)\n",
        "# y1\n",
        "\n",
        "print('test_weight=======================')\n",
        "print(test_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e735d350cea5"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir \"./logs\"\n",
        "# cmd 当前环境，当前目录 运行   tensorboard --logdir=logs --host=127.0.0.1\n",
        "# 网页中可以查看模型训练过程trainWindow, valWindow = proccess_data(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "#pd.concat([df17, df21, df22, df23, df24], axis=0, ignore_index=True)\n",
        "train, steps_per_epoch, feature_col_names, label_col_names = proccess_data_dataset(pd.concat([df21, df22, df23], axis=0, ignore_index=True), epochs, batch_size=batch_size)\n",
        "\n",
        "test, test_steps_per_epoch, test_feature_col_names, test_label_col_names = proccess_data_dataset(df17, epochs, batch_size=batch_size)\n",
        "\n",
        "# features_ndarray = None\n",
        "# labels_ndarray = None\n",
        "\n",
        "# for x, y in train_x, train_y:\n",
        "#     # 注意：实际应用中，特别是大数据集，直接转换整个数据集到ndarray可能会消耗大量内存\n",
        "#     features_ndarray = x.numpy()\n",
        "#     labels_ndarray = x.numpy()\n",
        "#     break  # 仅取一个batch作为示例，避免无限循环\n",
        "\n",
        "# print(\"Features NumPy Array Shape:\", features_ndarray.shape)\n",
        "# print(\"Labels NumPy Array Shape:\", labels_ndarray.shape)\n",
        "# print(steps_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "# 编译模型\n",
        "# %reload_ext autoreload\n",
        "# %autoreload 2\n",
        "# import keras.src\n",
        "from libs.compile_model import compile_model, get_callbacks\n",
        "\n",
        "feature_col_num = len(feature_col_names)\n",
        "print(feature_col_num)\n",
        "model = compile_model(time_steps, feature_col_num, class_num=class_num)\n",
        "initial_learning_rate = 0.01\n",
        "callbacks = get_callbacks(initial_learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "# tf.config.run_functions_eagerly(True)\n",
        "repeat_count = steps_per_epoch * epochs\n",
        "history = model.fit(\n",
        "    train.repeat(epochs * steps_per_epoch),\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    # batch_size=batch_size,\n",
        "    # batch_size=batch_size,#batch_size: Integer or None. Number of samples per gradient update.\n",
        "    # If unspecified, batch_size will default to 32.\n",
        "    # Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches).\n",
        "    # validation_data=test,\n",
        "    # validation_split=0.3,\n",
        "    # shuffle=False,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    # callbacks=callbacks,\n",
        "    class_weight=test_weight,\n",
        "    # workers=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history_metrics(history: keras.callbacks.History):\n",
        "    total_plots = len(history.history)\n",
        "    cols = total_plots // 2\n",
        "\n",
        "    rows = total_plots // cols\n",
        "\n",
        "    if total_plots % cols != 0:\n",
        "        rows += 1\n",
        "\n",
        "    pos = range(1, total_plots + 1)\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, (key, value) in enumerate(history.history.items()):\n",
        "        plt.subplot(rows, cols, pos[i])\n",
        "        plt.plot(range(len(value)), value)\n",
        "        plt.title(str(key))\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(log_dir, 'learning_rate.png'))\n",
        "\n",
        "plot_history_metrics(history) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存模型结构及配置参数\n",
        "model_json = model.to_json()\n",
        "with open(os.path.join(log_dir,'model_json.json'),'w') as json_file:\n",
        "    json_file.write(model_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# # print(len(valWindow.repeat(1)))\n",
        "# for train_x,y in test_feature.repeat(1).take(1):\n",
        "#     print(train_x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#对模型在测试集上进行评估\n",
        "metrics = model.evaluate(test.repeat(1))\n",
        "for name, value in zip(model.metrics_names, metrics):\n",
        "    print(name, ': ', value)\n",
        "# print(\"accuracy:\", metrics[0], \"accuracy:\", metrics[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_ds, test_steps_per_epoch, test_feature_col_names, test_label_col_names = proccess_data_dataset(df21, epochs, batch_size=batch_size)\n",
        "\n",
        "# pred_ds = tf.data.Dataset.from_tensor_slices((proccess_data_df(df21)[feature_col_names]))\n",
        "\n",
        "predictions = model.predict(pred_ds, verbose=\"auto\")\n",
        "# predictions?\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred =predictions.argmax(axis=1)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 如果是多分类问题并且想比较类别概率分布\n",
        "# 假设y_true和y_pred都是one-hot编码形式\n",
        "# 可以选择几个样本画柱状图对比\n",
        "\n",
        "\n",
        "y_true = test.map(lambda x, y: tf.argmax(y,axis=1))\n",
        "for i, v in enumerate(y_true):\n",
        "    if i > 3:\n",
        "        break\n",
        "    \n",
        "    # print(v)\n",
        "# print(predictions)\n",
        "\n",
        "# for i in y_pred:\n",
        "#     # print(i)\n",
        "#     # print(i)\n",
        "#     e=1\n",
        "# num_samples_to_plot = len(y_pred)\n",
        "# nrows = 2\n",
        "# plt.figure(figsize=(30, 6))\n",
        "# plt.subplot(nrows, 1, 1)\n",
        "# plt.plot(y_true, label='Actual values', linewidth=1.0)\n",
        "# plt.subplot(nrows, 1, 2)\n",
        "# plt.plot(y_pred, label='Predicted values', linewidth=1.0)\n",
        "# plt.legend(fontsize=14)\n",
        "# plt.xlabel('Sample Index')\n",
        "# ax = plt.gca()\n",
        "# ax.set_xlim(left=0, right=num_samples_to_plot)\n",
        "# plt.ylabel('Value')\n",
        "# plt.title('Actual vs Predicted Values Comparison')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "332c1750c3e9"
      },
      "outputs": [],
      "source": [
        "# # data.plot(legend=True, subplots=True, figsize=(30,30),  xlabel=\"steps\")\n",
        "# # ax = plt.gca()\n",
        "# # ax.set_xlim(left=0, right=19000)\n",
        "\n",
        "\n",
        "# # plt.show()\n",
        "# times = df22[time_col_names].index.to_numpy()\n",
        "# display(times)\n",
        "# plt.plot(times, y, label=\"Actual\", marker=\"o\")\n",
        "# plt.plot(times, predictions, label=\"Predicted\", linestyle=\"--\", marker=\"x\")\n",
        "\n",
        "# ax = plt.gca()\n",
        "# ax.set_xlim(left=12000, right=18000)\n",
        "# # 添加标题和标签\n",
        "# plt.title(\"Actual vs. Predicted Values\")\n",
        "# plt.xlabel(\"Sample Index\")\n",
        "# plt.ylabel(\"Value\")\n",
        "\n",
        "# # 添加图例\n",
        "# plt.legend()\n",
        "\n",
        "# # 显示图形\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3777035567f2"
      },
      "outputs": [],
      "source": [
        "# display(history.history)\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"model train vs validation loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"loss\", \"epoch\", \"accuracy\"], loc=\"upper right\")\n",
        "plt.show()\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"model train vs validation accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"accuracy\", \"val_accuracy\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9ca4cfdba43"
      },
      "outputs": [],
      "source": [
        "display(df22.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4072f6e1f1a6"
      },
      "outputs": [],
      "source": [
        "# data22, scaler22 = normalize(df22, numeric_col_names)\n",
        "# X22 = data22[feature_col_names]\n",
        "# y_test22 =normalizeY( df22[label_col_names])\n",
        "# x_train22  = np.asarray(X22) .reshape(-1, feature_col_num)\n",
        "# predY22 = model.predict(x_train22, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# data23, scaler23 = normalize(df23, numeric_col_names)\n",
        "# X23 = data23[feature_col_names]\n",
        "# y_test23 =normalizeY( df23[label_col_names])\n",
        "# x_train23  = np.asarray(X23) .reshape(-1, feature_col_num)\n",
        "# predY23 = model.predict(x_train23, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# display(predY22,predY23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e692ce96c13"
      },
      "outputs": [],
      "source": [
        "# fig_acc = plt.figure(figsize=(9, 9))\n",
        "# plt.plot(predY23[0:27000])\n",
        "# plt.plot(y_test23[0:27000])\n",
        "# plt.title(\"real vs pred\")\n",
        "# plt.ylabel(\"value\")\n",
        "# plt.xlabel(\"epoch\")\n",
        "# plt.xlim((19000, 23000))\n",
        "# plt.legend([\"pred\", \"real\"], loc=\"upper left\")\n",
        "# plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "C_L_A_train_test.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
